{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Respuestas de la práctica Calificada de Inteligencia Artificial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista de ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Traduzca la siguiente expresión lógica de descripción a lógica de primer orden y explica el resultado:\n",
    "\n",
    "```\n",
    "And(Man, AtLeast (3, Son), AtMost(2, Daughter ),\n",
    "    All(Son, And(Unemployed , Married , All(Spouse, Doctor ))),\n",
    "    All(Daughter , And(Professor , Fills(Department , Physics, Math)))).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "La información dice \"Todos los hombres con al menos tres hijos que están desempleados y casados con médicos y al menos dos hijas que son profesores en los departamentos de física o matemáticas\"\n",
    "\n",
    "$Man(x) \\wedge MinimumOf(x, 3, Sons) \\wedge MaximumOf(x, 2, Daughters)$\n",
    "\n",
    "$SonOf(y,x) \\Rightarrow Unemployed(y) \\wedge MarriedTo(y,z) \\wedge IsDoctor(z)$\n",
    "\n",
    "$DaughterOf(w,x) \\Rightarrow IsProfessor(w) \\wedge (InMathDept(w) \\vee InPhysicsDept(w))$\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ejecuta el descenso de gradiente para minimizar la función:\n",
    "\n",
    "$$g(w) = \\frac{1}{50}(w^4 + w^2 +10w)$$\n",
    "\n",
    "con un punto inicial $w_0 = 2$ y $1000$ iteraciones. Haga tres corridas separadas usando cada uno de los valores del tamaño de paso $\\lambda = 1$, $\\lambda = 10^{-1}$ y $\\lambda = 10^{-2}$. Calcula la derivada de esta función a mano e implementa esto (así como la función ) en Python usando NumPy.\n",
    "\n",
    "Traza la gráfica del historial de la función de costo resultante de cada ejecución en una sola figura para comparar su desempeño. ¿Qué valor del tamaño de paso funciona mejor para esta función y punto inicial en particular?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(alpha,max_its,w):\n",
    "    g = lambda w: 1/50*(w**4 + w**2 + 10*w)\n",
    "    \n",
    "    grad = lambda w: 1/50*(4*w**3 + 2*w + 10)\n",
    "\n",
    "    cost_history = [g(w)]        \n",
    "    for k in range(1,max_its+1):       \n",
    "        grad_eval = grad(w)\n",
    "\n",
    "        w = w - alpha*grad_eval\n",
    "            \n",
    "        cost_history.append(g(w))  \n",
    "    return cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2.0\n",
    "max_its = 1000\n",
    "\n",
    "alpha = 10**(0)\n",
    "cost_history_1 = gradient_descent(alpha,max_its,w)\n",
    "\n",
    "alpha = 10**(-1)\n",
    "cost_history_2 = gradient_descent(alpha,max_its,w)\n",
    "\n",
    "alpha = 10**(-2)\n",
    "cost_history_3 = gradient_descent(alpha,max_its,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Respuesta4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En clase hemos tratada la regresión ridge ($l_2$-regularizada) y $l_1$-regularizada (lasso). Existe una versión híbrida llamada *elástic net* que usa términos de regularización $l_1$ y $l_2$:\n",
    "\n",
    "$$J_{EL} = \\Vert \\mathbf{y} -\\mathbf{Xw} \\Vert^2 + \\lambda_1 \\Vert \\mathbf{w} \\Vert_1 + \\lambda_2 \\Vert \\mathbf{w} \\Vert^2$$\n",
    "\n",
    "    Si definimos:\n",
    "\n",
    "$$J_{2} = \\Vert \\mathbf{\\tilde{y}} -\\mathbf{\\tilde{X}w} \\Vert^2  + c\\lambda_1 \\Vert \\mathbf{w} \\Vert_1$$\n",
    "\n",
    "donde $c = (1 + \\lambda_2)^{-1/2}$ y \n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde{X}} = \\begin{pmatrix}\n",
    "\\mathbf{X}\\\\\n",
    "\\sqrt{\\lambda_2}\\mathbf{I}_d\n",
    "\\end{pmatrix}, \\qquad \\mathbf{\\tilde{y}} =  \\begin{pmatrix}\n",
    "\\mathbf{y}\\\\\n",
    "\\mathbf{0}_{d \\times 1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Muestra que: \n",
    "\n",
    "$$arg \\min_{\\mathbf{w}}J_{EL}(\\textbf{w}) = c(arg \\min_{\\mathbf{w}}J_2(\\mathbf{w})))$$.\n",
    "\n",
    "Esto implica que un problema de *elastic net* puede resolverse como un problema de *lasso*, utilizando datos modificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Respuesta1.png)\n",
    "\n",
    "<font color='red'>Realizando el cambio $\\tilde{w} = c^{-1}w:$ </font>\n",
    "\n",
    "![](Respuesta2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explica los siguientes resultados indicando verdadero o falso. Se puntuara si se contesta todas los ítems.\n",
    "\n",
    " * Para la regresión logística, con parámetros optimizados mediante un método de gradiente estocástico, establecer los parámetros en $0$ es una inicialización aceptable.\n",
    "\n",
    " * El uso de la validación cruzada para seleccionar hiperparámetros garantizará que nuestro modelo no se sobreajuste.\n",
    "\n",
    " * Los algoritmos Bagging asignan pesos $w_1 \\dots, w_n$ a un conjunto de $N$ estudiantes débiles. Vuelven a ponderar a los alumnos y los convierten en fuertes. Los algoritmos Boosting extraen $N$ distribuciones de muestra (generalmente con reemplazo) de un conjunto de datos original para que los alumnos se entrenen.\n",
    "\n",
    " * Un árbol de decisión binario de profundidad infinita siempre puede alcanzar el $100\\%$ de exactitud en el entrenamiento, siempre que ningún punto esté mal etiquetado en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    \n",
    "* Esa afirmación computacional sobre la regresión logística clave. \n",
    "* La validación cruzada no garantiza que un modelo no se sobreajuste, pero puede ayudar a identificar un caso de sobreajuste.\n",
    "\n",
    "* Esta aseveración es falsa y el Bagging ofrece la ventaja de permitir que muchos estudiantes débiles combinen esfuerzos para superar a un solo estudiante fuerte.\n",
    "\n",
    "* Esta afirmación es verdadera.Obtienes un $100\\%$ de exactitud porque estás usando una parte de los datos de entrenamiento para las pruebas. En el momento del entrenamiento, el árbol de decisiones adquirió el conocimiento sobre esos datos y ahora, si se proporciona los mismos datos para predecir, dará el mismo valor. Es por eso que el árbol de decisiones produce resultados correctos en todo momento.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Se podría perforar un pozo de petróleo en la granja del profesor Neapolitan en Texas. Con base en lo que ha sucedido en granjas similares, juzgamos que la probabilidad de que haya petróleo presente es de $0.5$, la probabilidad de que solo esté presente gas natural es de $0.2$ y la probabilidad de que ninguno de los dos esté presente es de $0.3$. Si hay petróleo presente, una prueba geológica dará un resultado positivo con probabilidad de $0.9$,  si solo hay gas natural, dará un resultado positivo con probabilidad $0.3$  y si ninguno está presente, la prueba será positiva con probabilidad $0.1$. Supongamos que la prueba resulta positiva. Utilice el teorema de Bayes para calcular la probabilidad de que haya petróleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&P(Presente|Positivo) ?? \\\\\n",
    "&= \\frac{P(Positivo|Presente)P (Presente)}{P(Positivo|Presente)P(Presente) + P (Positivo|Gas)P(Gas) + P(Positivo|N)P(N)} \\\\\n",
    "&= \\frac{(0.9)(0.5)}{(0.9)(0.5) + (0.3)(0.2) + (0.1)(0.3)} = 0.833\n",
    "\\end{align*}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 . Se te proporciona un modelo de Bayes, que se muestra a continuación, con la etiqueta $Y$ y las características $X_1$ y $X_2$. Las probabilidades condicionales del modelo están parametrizadas por $p_1$, $p_2$ y $q$.\n",
    "\n",
    "![](NaiveBayes.png)\n",
    "\n",
    "Ten en cuenta que algunos de los parámetros son compartidos (por ejemplo, $P(X_1 = 0|Y = 0)=P(X_1 = 1|Y = 1) = p_1$).\n",
    "\n",
    "\n",
    "Dado un nuevo punto de dato con $X_1 = 1$ y $X_2 = 1$, ¿cuál es la probabilidad de que este punto tenga la etiqueta $Y = 1$? Expresa tu respuesta en términos de los parámetros $p_1$, $p_2$ y $q$ (es posible que no los necesite todos). Es decir debes calcular : $P(Y= 1| X_1= 1,X_2= 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Respuesta3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explica los siguientes resultados indicando verdadero o falso. Se puntuara si se contesta todas los ítems.\n",
    "\n",
    "  * La acción realizada por un agente racional siempre será una función determinista de las percepciones actuales del agente.\n",
    "\n",
    " * Una ruta de solución óptima para un problema de búsqueda con costos positivos nunca tendrá estados repetidos.\n",
    "\n",
    " * Si dos heurísticas de búsqueda $h_1(s)$ y $h_2(s)$ tienen el mismo valor promedio, la heurística $h_3(s) = \\max (h_1(s), h_2(s))$ podría dar una mejor eficiencia $A^*$ que $h_1$ o $h_2$.\n",
    "\n",
    " * Para cualquier conjunto de atributos, y cualquier conjunto de entrenamiento generado por una función determinista para esos atributos, existe un árbol de decisiones que es consistente con ese conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> \n",
    "    \n",
    "* Falso. Primero, las funciones no deterministas son útiles en muchos entornos, como los entornos de múltiples agentes. En segundo lugar, el agente a menudo debe considerar su historial de percepción y otras fuentes de información.\n",
    "\n",
    "* Verdadero. Para cualquier ruta de solución con estados repetidos, eliminar el ciclo también será una ruta de solución con un costo menor.\n",
    "\n",
    "* Verdadero. $h_1$ podría tener valores exactos para estados anteriores  y $h_2$ tener valores exactos para estados posteriores. $h_3$ podría tener valores más exactos que cualquiera de los componentes heurísticos.\n",
    "\n",
    "* Verdadero. Los árboles de decisión pueden representar cualquier función determinista.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 . Usaremos el conjunto de datos [*Breast Cancer Wisconsin (Diagnostic)*](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/version/2), llamado `data.csv`\n",
    "\n",
    " * ¿Cuántas observaciones (filas) hay en el conjunto de datos?\n",
    " * Asigna a una lista de nombre `feature_columns` todas las columnas del conjunto de datos menos el `id` de cada observación y la clase `diagnosis` que es la que buscaremos predecir.\n",
    " * Crea una nueva columna `'target'` que tenga un valor numérico de `+1` en las muestras positivas (cuando el diagnóstico sea maligno) y `-1` en las muestra negativas (diagnóstico benigno).\n",
    " * Empleando `sklearn.model_selection.train_test_split` separa el conjunto de datos en un 90% para entrenamiento y validación (`X_trainval`, `y_trainval`), y 10% para pruebas (`X_test`, `y_test`).\n",
    "\n",
    "   Luego separa (`X_trainval`, `y_trainval`) en un 80% para entrenamiento (`X_train`, `y_train`) y 20% para validación (`X_val`, `y_val`).\n",
    "   \n",
    "* Empleando `sklearn.preprocessing.StandardScaler` se ha normalizado en un arreglo `X_trainval_scaled` el conjunto de entrenamiento y validación. Usa `sklearn.decomposition.PCA` para calcular los vectores de carga **`pca_loadings`** y el puntaje **`pca_scores`** de cada observación en el espacio de los componentes principales.\n",
    "\n",
    "* Crea una instancia de la clase `sklearn.linear_model.LogisticRegression` y ajusta un modelo con el conjunto de **entrenamiento**\n",
    "\n",
    "* Usa el modelo para predecir la probabilidad de que cada una de las observaciones de conjunto de **validación** corresponda a la clase positiva (diagnóstico maligno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de observaciones es 569\n"
     ]
    }
   ],
   "source": [
    "numero_de_observaciones =data.shape[0]\n",
    "print('El número de observaciones es {}'.format(numero_de_observaciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nombre de todas las columnas es: \n",
      " ['id' 'diagnosis' 'radius_mean' 'texture_mean' 'perimeter_mean'\n",
      " 'area_mean' 'smoothness_mean' 'compactness_mean' 'concavity_mean'\n",
      " 'concave points_mean' 'symmetry_mean' 'fractal_dimension_mean'\n",
      " 'radius_se' 'texture_se' 'perimeter_se' 'area_se' 'smoothness_se'\n",
      " 'compactness_se' 'concavity_se' 'concave points_se' 'symmetry_se'\n",
      " 'fractal_dimension_se' 'radius_worst' 'texture_worst' 'perimeter_worst'\n",
      " 'area_worst' 'smoothness_worst' 'compactness_worst' 'concavity_worst'\n",
      " 'concave points_worst' 'symmetry_worst' 'fractal_dimension_worst'\n",
      " 'Unnamed: 32']\n",
      "\n",
      "Las columnas con características son: \n",
      " ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "all_columns = data.columns.values\n",
    "print('El nombre de todas las columnas es: \\n', all_columns)\n",
    "print()\n",
    "\n",
    "data.drop(\"id\",axis=1)\n",
    "data.drop(\"diagnosis\",axis=1)\n",
    "\n",
    "feature_columns = list(data.columns[2:32])\n",
    "print('Las columnas con características son: \\n', feature_columns)\n",
    "\n",
    "assert 'id' not in feature_columns, 'id no debe estar en feature_columns'\n",
    "assert 'diagnosis' not in feature_columns, 'diagnosis no debe estar en feature_columns'\n",
    "assert len(feature_columns) == 30, 'Parece que falta columnas de características'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original de la columna diagnosis:\n",
      "B    357\n",
      "M    212\n",
      "Name: diagnosis, dtype: int64\n",
      "\n",
      "Distribución de la nueva columna target:\n",
      "-1    357\n",
      " 1    212\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Distribución original de la columna diagnosis:')\n",
    "print(data['diagnosis'].value_counts())\n",
    "\n",
    "data['target'] = np.where(data['diagnosis']=='M', 1, -1)\n",
    "\n",
    "\n",
    "print('\\nDistribución de la nueva columna target:')\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "assert ( data['diagnosis'].value_counts().values \n",
    "         == data['target'].value_counts().values\n",
    "       ).all(), 'La distribución de target debería coincidir con la de diagnosis'\n",
    "assert -1 in data['target'].values, 'No se encontró valores -1 en la columna target'\n",
    "assert 1 in data['target'].values, 'No se encontró valores +1 en la columna target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[feature_columns]\n",
    "y = data['target']\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X,y, test_size = 0.1)\n",
    "\n",
    "assert X_trainval.shape == (512, 30), 'Dimensiones incorrectas'\n",
    "assert X_test.shape == (57, 30), 'Dimensiones incorrectas'\n",
    "assert X_trainval.shape[0] == y_trainval.shape[0], 'Dimensiones incorrectas'\n",
    "assert X_test.shape[0] == y_test.shape[0], 'Dimensiones incorrectas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train, X_val, y_val = train_test_split(X_trainval, y_trainval,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler().fit(X_trainval)\n",
    "X_trainval_scaled = scaler.transform(X_trainval)\n",
    "\n",
    "pca = PCA().fit(X_trainval_scaled)\n",
    "pca_loadings = pca.components_\n",
    "pca_scores = pca.transform(X_trainval_scaled)\n",
    "\n",
    "assert np.isclose(X_trainval_scaled, pca_scores @ pca_loadings).all(), 'Error al calcular pca_scores o pca_loadings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_trainval_scaled, y_trainval)\n",
    "assert type(modelo) == LogisticRegression, 'Falta instanciar debidamente el modelo'\n",
    "assert modelo.classes_.shape == (2,), 'Falta ajustar el modelo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = modelo.predict(X_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 318],\n",
       "       [  0, 194]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_trainval, y_pred_val)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
