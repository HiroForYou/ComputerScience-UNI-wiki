{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Curso Inteligencia Artificial\n",
    "\n",
    "El presente texto ha sido preparado de manera exclusiva para los alumnos del Curso de Inteligencia\n",
    "Artificial, que forma parte de la Plan de Estudio de la Escuela de Ciencia de Computación, según el artículo\n",
    "°44 de la Ley sobre el Derecho de Autor, D.L. N°822. Queda prohibida su difusión y reproducción por\n",
    "cualquier medio o procedimiento, total o parcialmente fuera del marco del presente curso.\n",
    "\n",
    "### Curvas de exhaustividad- precisión y curvas ROC\n",
    "\n",
    "Cambiar el umbral que se usa para tomar una decisión de clasificación en un modelo es una forma de ajustar la compensación de precisión y exhaustividad para un clasificador dado. Esta decisión depende de la aplicación y debe ser impulsada por los objetivos comerciales. Una vez que se establece un objetivo en particular, es decir, una exhaustividad particular o un valor de precisión para una clase, se puede establecer un umbral de manera apropiada. Siempre es posible establecer un umbral para cumplir un objetivo en particular, como un $90\\%$ de exhaustividad. La parte difícil es desarrollar un modelo que todavía tenga una precisión razonable con este umbral: si clasifica todo como positivo, tendrá un $100\\%$ de exhaustividad, pero su modelo será inútil.\n",
    "\n",
    "Establecer un requisito en un clasificador como $90\\%$ de exhaustividad a menudo se llama establecer un *punto de operación*. El ajuste  de un punto de operación suele ser útil en la configuración empresarial para garantizar el rendimiento a los clientes u otros grupos dentro de tu organización.\n",
    "\n",
    "A menudo, al desarrollar un nuevo modelo, no está del todo claro cuál será el punto de operación. Por esta razón y para comprender mejor un problema de modelado, es instructivo observar todos los umbrales posibles, o todos los posibles intercambios de precisión y exhaustividad a la vez. Esto es posible usando una herramienta llamada curva de precision-exhaustividad. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotsMetrics import *\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "X, y = hacer_blobs(n_muestras=(400, 50), centros=2, cluster_std=[7.0, 2],\n",
    "                      random_state=22)\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_entrenamiento, y_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, exhaustividad, umbrales = precision_recall_curve(y_prueba, svc.decision_function(X_prueba))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `precision_recall_curve` devuelve una lista de valores de precisión y exhaustividad para todos los umbrales posibles (todos los valores que aparecen en la función de decisión) en orden, para que podamos dibujar una curva, como se ve en la siguiente figura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = hacer_blobs(n_muestras=(4000, 500), centros=2, cluster_std=[7.0, 2],random_state=22)\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_entrenamiento, y_entrenamiento)\n",
    "precision,exhaustividad, umbrales = precision_recall_curve(y_prueba, svc.decision_function(X_prueba))\n",
    "\n",
    "# Encontramos umbrales cercanos a cero\n",
    "cerca_cero = np.argmin(np.abs(umbrales))\n",
    "plt.plot(precision[cerca_cero], exhaustividad[cerca_cero], 'o', markersize=10,\n",
    "label=\"umbral cero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot(precision, exhaustividad, label=\"Curva precision-exhaustividad\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Exhaustividad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada punto a lo largo de la curva en la figura anterior corresponde a un umbral posible de `decision_function`. Podemos ver, por ejemplo, que podemos lograr una exhaustividad de $0.4$ con una precisión de aproximadamente $0.75$. El círculo negro marca el punto que corresponde a un umbral de $0$, el umbral predeterminado para `decision_function`. Este punto es el término medio que se elige al llamar al método `predict`.\n",
    "\n",
    "Cuanto más cerca esté una curva de la esquina superior derecha, mejor será el clasificador. Un punto en la esquina superior derecha significa alta precisión y alta exhaustividad  para el mismo umbral. La curva comienza en la esquina superior izquierda, que corresponde a un umbral muy bajo, clasificando todo como  clase positiva. Elevar el umbral mueve la curva hacia una mayor precisión, pero también a una exhaustividad menor. Al elevar el umbral cada vez más, llegamos a una situación en la que la mayoría de los puntos clasificados como positivos son verdaderos positivos, lo que conduce a una precisión muy alta pero a una baja exhaustividad. Cuanto más mantengas la exhaustividad alta a medida que aumenta la precisión es mejor.\n",
    "\n",
    "Al observar esta curva particular un poco más, podemos ver que con este modelo es posible obtener una precisión de hasta $0.5$ con una exahautividad muy alta. Si queremos una precisión mucho mayor, tenemos que sacrificar una gran cantidad de exhaustividad. En otras palabras, a la izquierda, la curva es relativamente plana, lo que significa que la exhaustividad no baja mucho cuando se requiere una mayor precisión. Para una precisión superior a $0.5$, cada ganancia en precisión nos cuesta mucha exhaustividad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Diferentes clasificadores pueden funcionar bien en diferentes partes de la curva, es decir, en diferentes puntos de operación. Comparemos el SVM que entrenamos con un bosque aleatorio entrenado en el mismo conjunto de datos. El `RandomForestClassifier` no tiene `decision_function`, solo `predict_proba`. La función `precision_recall_curve` espera como segundo argumento una medida de certeza para la clase positiva (clase $1$), por lo que pasamos la probabilidad de que una muestra sea de clase $1$, es decir, `rf.predict_proba (X_prueba) [:,1]`. \n",
    "\n",
    "El umbral predeterminado para `predic_proba` en la clasificación binaria es $0.5$, por lo que este es el punto que marcamos en la curva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_entrenamiento, y_entrenamiento)\n",
    "\n",
    "precision_rf, exhaustividad_rf, umbrales_rf = precision_recall_curve(\n",
    "    y_prueba, rf.predict_proba(X_prueba)[:, 1])\n",
    "plt.plot(precision, exhaustividad, label=\"svc\")\n",
    "\n",
    "plt.plot(precision[cerca_cero], exhaustividad[cerca_cero], 'o', markersize=10,\n",
    "                   label=\"umbral cero  svc\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot(precision_rf, exhaustividad_rf, label=\"rf\")\n",
    "\n",
    "cerca_estandar_rf = np.argmin(np.abs(umbrales_rf - 0.5))\n",
    "plt.plot(precision_rf[cerca_estandar_rf], exhaustividad_rf[cerca_estandar_rf], '^', c='k',\n",
    "         markersize=10, label=\"umbral 0.5 rf\", fillstyle=\"none\", mew=2)\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Exhaustividad\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del gráfico de comparación, podemos ver que el bosque aleatorio tiene un mejor rendimiento en los extremos, para una exhaustividad  muy alta o requisitos de muy alta precisión. Alrededor del medio (aproximadamente precisión = $0.7$), el SVM funciona mejor. Si solo miramos el f1-score para comparar el desempeño general, nos perderíamos estas sutilezas. El f1-score solo captura un punto en la curva de precisión-exhaustividad, el que viene dado por el umbral predeterminado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score de bosques aleatorios: {:.3f}\".format(\n",
    "    f1_score(y_prueba, rf.predict(X_prueba))))\n",
    "print(\"f1_score de svc: {:.3f}\".format(f1_score(y_prueba, svc.predict(X_prueba))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparación de dos curvas de precisión-exhaustividad  proporciona una gran cantidad de información detallada, pero es un proceso manual. Para la comparación automática de un modelo, podríamos resumir la información contenida en la curva, sin limitarnos a un umbral o punto de operación particular. Una forma particular de resumir la curva de precisión-exhaustividad es calcular la integral o el área bajo la curva de precisión-exhaustividad, también conocida como precisión promedio.\n",
    "\n",
    "Se puede usar la función `average_precision_score` para calcular la precisión promedio. Debido a que necesitamos calcular la curva ROC y considerar múltiples umbrales, el resultado de `decision_function` o `predic_proba` debe pasarse a `average_precision_score`, no el resultado de `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(y_prueba, rf.predict_proba(X_prueba)[:, 1])\n",
    "ap_svc = average_precision_score(y_prueba, svc.decision_function(X_prueba))\n",
    "print(\"Precision promedio de bosques aleatorios: {:.3f}\".format(ap_rf))\n",
    "print(\"Precision promedio de svc: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al promediar todos los umbrales posibles, vemos que el bosque aleatorio y el SVC funcionan de manera similar. Esto es bastante diferente del resultado que obtuvimos de `f1_score` anteriormente. Debido a que la precisión promedio es el área bajo una curva que va de $0$ a $1$, la precisión promedio siempre devuelve un valor entre $0$ (peor) y $1$ (mejor). La precisión promedio de un clasificador que asigna `decision_function` al azar es la fracción de muestras positivas en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características operativa del receptor (ROC) y AUC\n",
    "\n",
    "Hay otra herramienta que se usa comúnmente para analizar el comportamiento de los clasificadores en diferentes umbrales: la curva de características operativas del receptor o la curva ROC para abreviar. De forma similar a la curva de precisión-exhaustividad, la curva ROC considera todos los umbrales posibles para un clasificador dado, pero en lugar de informar la precisión y la exhaustividad, muestra la tasa de falsos positivos (FPR) frente a la tasa de positivos verdaderos (TPR). \n",
    "\n",
    "Debes recordar  que la tasa positiva verdadera es simplemente otro nombre para exhaustividad, mientras que la tasa de falsos positivos es la fracción de falsos positivos de todas las muestras negativas:\n",
    "\n",
    "$FPR = \\frac{FP}{FP +TN}$\n",
    "\n",
    "La curva ROC, puede ser calculada por la función `roc_curve`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, umbrales = roc_curve(y_prueba, svc.decision_function(X_prueba))\n",
    "plt.plot(fpr, tpr, label=\"Curva ROC \")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (exhaustividad)\")\n",
    "\n",
    "# Encontramos umbrales cercanos a cero \n",
    "cerca_cero = np.argmin(np.abs(umbrales))\n",
    "plt.plot(fpr[cerca_cero], tpr[cerca_cero], 'o', markersize=10,\n",
    "label=\"umbral cero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la curva ROC, la curva ideal está cerca de la esquina superior izquierda: se desea un clasificador que produzca un alto nivel de exhaustividad mientras mantiene una baja tasa de falsos positivos. En comparación con el umbral predeterminado de $0$, la curva muestra que podemos lograr una exhaustividad significativamente mayor (alrededor de $0.9$) al tiempo que aumentamos ligeramente la FPR.\n",
    "\n",
    "El punto más cercano a la esquina superior izquierda podría ser un mejor punto de operación que el elegido por defecto. De nuevo, ten en cuenta que la elección de un umbral no se debe hacer en el conjunto de prueba, sino en un conjunto de validación por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_rf, tpr_rf, umbrales_rf = roc_curve(y_prueba, rf.predict_proba(X_prueba)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Curva de ROC SVC\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"Curva ROC RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (exhaustividad)\")\n",
    "\n",
    "plt.plot(fpr[cerca_cero], tpr[cerca_cero], 'o', markersize=10,\n",
    "label=\"umbral cero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "cerca_estandar_rf = np.argmin(np.abs(umbrales_rf - 0.5))\n",
    "plt.plot(fpr_rf[cerca_estandar_rf], tpr[cerca_estandar_rf], '^', markersize=10,\n",
    "label=\"umbral 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la curva  precisión-exhaustividad, a menudo queremos resumir la curva ROC usando un solo número, el área bajo la curva (esto comúnmente se conoce como el AUC, y se entiende que la curva en cuestión es la curva ROC ) Podemos calcular el área bajo la curva ROC usando la función `roc_auc_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_prueba, rf.predict_proba(X_prueba)[:, 1])\n",
    "svc_auc = roc_auc_score(y_prueba, svc.decision_function(X_prueba))\n",
    "print(\"AUC para bosques aleatorios: {:.3f}\".format(rf_auc))\n",
    "print(\"AUC para SVC: {:.3f}\".format(svc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando el bosque aleatorio y SVM usando el puntaje AUC, encontramos que el bosque aleatorio se comporta bastante mejor que el SVM. Recuerda que, dado que la precisión promedio es el área bajo una curva que va de $0$ a $1$, la precisión promedio siempre devuelve un valor entre $0$ (peor) y $1$ (mejor). La predicción aleatoria siempre produce un AUC de $0.5$, sin importar qué tan desbalanceadas estén las clases en un conjunto de datos. Esto hace que AUC sea una métrica mucho mejor para los problemas de clasificación desbalanceadas que la precisión.\n",
    "\n",
    "El AUC puede interpretarse como una evaluación del ranking de muestras positivas. Es equivalente a la probabilidad de que un punto escogido al azar de la clase positiva tenga una puntuación más alta de acuerdo con el clasificador que un punto escogido al azar de la clase negativa. Por lo tanto, un AUC perfecto de $1$ significa que todos los puntos positivos tienen una puntuación más alta que todos los puntos negativos. Para problemas de clasificación con clases desbalanceadas, el uso de AUC para la selección del modelo a menudo es mucho más significativo que el uso de la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Clasifiquemos  todos los nueves en el conjunto de datos de `digits` frente a todos los demás dígitos. Clasificaremos el conjunto de datos con un SVM con tres configuraciones diferentes del ancho de banda del kernel, `gamma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digitos = load_digits()\n",
    "y = digitos.target == 9\n",
    "\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(\n",
    "digitos.data, y, random_state=0)\n",
    "plt.figure()\n",
    "for gamma in [1, 0.05, 0.01]:\n",
    "    svc = SVC(gamma=gamma).fit(X_entrenamiento, y_entrenamiento)\n",
    "    precision = svc.score(X_prueba, y_prueba)\n",
    "    \n",
    "    auc = roc_auc_score(y_prueba, svc.decision_function(X_prueba))\n",
    "    fpr, tpr, _ = roc_curve(y_prueba , svc.decision_function(X_prueba))\n",
    "    print(\"gamma = {:.2f} precision = {:.2f} AUC = {:.2f}\".format(gamma, precision, auc))\n",
    "    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.02)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de las tres configuraciones de gamma es el $90\\%$. Esto podría ser lo mismo que un rendimiento aleatorio. Sin embargo, al observar el AUC y la curva correspondiente, vemos una clara distinción entre los tres modelos. Con gamma = $1.0$, el AUC está realmente en el nivel aleatorio, lo que significa que el resultado de `decision-function` es tan bueno como aleatorio. Con gamma = $0.05$, el rendimiento mejora drásticamente a un AUC de $0.5$. Finalmente, con gamma = $0.01$, obtenemos un AUC perfecto de $1.0$. Eso significa que todos los puntos positivos se clasifican por encima de todos los puntos negativos según la función de decisión. \n",
    "\n",
    "En otras palabras, con el umbral correcto, este modelo puede clasificar los datos perfectamente. Sabiendo esto, podemos ajustar el umbral en este modelo y obtener grandes predicciones. Si solo hubiéramos utilizado la precisión, nunca hubiéramos descubierto esto.  Por esta razón, se recomienda encarecidamente el uso de AUC al evaluar modelos en datos desbalanceados. Se debe tener  en cuenta que AUC no hace uso del umbral predeterminado, por lo que puede ser necesario ajustar el umbral de decisión para obtener resultados de clasificación útiles de un modelo con un AUC alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para clasificación multiclase\n",
    "\n",
    "Todas las métricas para la clasificación multiclase se derivan de las métricas de clasificación binaria, pero se promedian en todas las clases. La exactitud de la clasificación multiclase se define nuevamente como la fracción de ejemplos correctamente clasificados. Y nuevamente, cuando las clases están desbalanceadas, la exactitud no es una gran medida de evaluación.\n",
    "\n",
    "Imaginemos un problema de clasificación de tres clases con un $85\\%$ de puntos pertenecientes a la clase A, un $10\\%$ perteneciente a la clase B y un $5\\%$ perteneciente a la clase C. ¿Qué significa tener un $85\\%$ de exactitud en este conjunto de datos? En general, los resultados de la clasificación multiclase son más difíciles de entender que los resultados de la clasificación binaria.\n",
    "\n",
    "Además de la exactitud, en estos casos, las herramientas comunes son la matriz de confusión y el reporte de clasificación que vimos en el caso binario. Vamos a aplicar estos dos métodos de evaluación detallados en la tarea de clasificar los $10$ dígitos manuscritos diferentes en el conjunto de datos `digits`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(\n",
    "digitos.data, digitos.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression(max_iter=10000).fit(X_entrenamiento, y_entrenamiento)\n",
    "pred = lr.predict(X_prueba)\n",
    "print(\"Precision: {:.3f}\".format(accuracy_score(y_prueba, pred)))\n",
    "print(\"Matriz de confusion:\\n{}\".format(confusion_matrix(y_prueba, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene una exactitud del $95.3\\%$, lo que nos dice que lo estamos haciendo bastante bien. La matriz de confusión nos proporciona más detalles. En cuanto al caso binario, cada fila corresponde a una etiqueta verdadera y cada columna corresponde a una etiqueta predicha. Se puede encontrar un gráfico  visualmente más informativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen_puntuacion = mapa_calor(confusion_matrix(y_prueba, pred), xetiqueta='Etiqueta predecida',\n",
    "                               yetiqueta='Etiqueta verdadera', xticklabels=digitos.target_names,\n",
    "                                yticklabels=digitos.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\n",
    "plt.title(\"Matriz de confusion\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la primera clase, el dígito $0$, hay $37$ muestras en la clase  y todas estas muestras se clasificaron como clase 0 (no hay falsos negativos para la clase $0$). Podemos ver eso porque todas las demás entradas en la primera fila de la matriz de confusión son $0$. También podemos ver que ningún otro dígito se clasificó erróneamente como $0$, porque todas las demás entradas en la primera columna de la matriz de confusión son $0$ (no hay falsos positivos para la clase $0$). Sin embargo, algunos dígitos se confundieron con otros, por ejemplo, el dígito $2$ (tercera fila), tres de los cuales se clasificaron como el dígito $3$ (cuarta columna). También hubo un dígito $3$ que se clasificó como $2$ (tercera columna, cuarta fila) y un dígito $8$ que se clasificó como $2$ (columna tercera, cuarta fila).\n",
    "\n",
    "Con la función `classification_report`, podemos calcular la precisión, exhaustividad y `f-score` para cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_prueba, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, la precisión y la exhaustividad son un $1$ perfecto para la clase $0$, ya que no hay confusiones con esta clase. Para la clase $7$, por otro lado, la precisión es $1$, porque ninguna otra clase se clasificó erróneamente como $7$, mientras que para la clase $6$, no hay falsos negativos, por lo que la exhaustividad es $1$. También podemos ver que el modelo tiene dificultades particulares con clases $8$ y $3$.\n",
    "\n",
    "La métrica  comúnmente utilizada para los conjuntos de datos desbalanceados  en la configuración de multiclases es la versión multiclase de `f-score`. La idea detrás del `f-scale` multiclase es calcular un `f-score` binario por clase, siendo esa clase la clase positiva y las otras clases conformando  las clases negativas. Luego, estas `f-score` por clase se promedian utilizando una de las siguientes estrategias:\n",
    "\n",
    "* El promedio \"macro\", calcula los `f-score` por clase no ponderados. Da igual peso para todas las clases, sin importar su tamaño.\n",
    "* El promedio \"weighted \", calcula la media de los `f-score` por clase, ponderados por su soporte. Esto es lo que se indica  en el informe de clasificación.\n",
    "\n",
    "* El promedio \"micro\" calcula la cantidad total de falsos positivos, falsos negativos y verdaderos positivos en todas las clases, y luego calcula la precisión, la exhaustividad y el `f-score ` usando este valor.\n",
    "\n",
    "\n",
    "Si te importa cada muestra por igual, se recomienda utilizar el promedio \"micro\", si te preocupa por cada clase por igual, se recomienda utilizar el promedio macro\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Promedio micro f1-score: {:.3f}\".format(f1_score(y_prueba, pred, average=\"micro\")))\n",
    "print(\"Promedio macro f1-score: {:.3f}\".format(f1_score(y_prueba, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de regresión\n",
    "\n",
    "La evaluación para la regresión se puede hacer con detalles similares a los de la clasificación, por ejemplo, analizando la sobrepredicción  del objetivo versus la predicción insuficiente del objetivo. Sin embargo, en la mayoría de las aplicaciones que hemos visto, usar el $R^2$ predeterminado utilizado en el método `score` de todos los regresores es suficiente. En ocasiones, las decisiones comerciales se toman sobre la base de un error cuadrático medio o un error absoluto medio, lo que podría incentivar a ajustar  los modelos que utilizan estas métricas. Sin embargo, en general, hemos encontrado tambien que $R^2$ es una medida más intuitiva para evaluar los modelos de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de métricas de evaluación en la selección de modelos\n",
    "\n",
    "A menudo podemos usar métricas como AUC en la selección de modelos usando `GridSearchCV` o `cross_val_score`. scikit-learn proporciona una forma muy simple de lograr esto, a través del argumento `scoring` que se puede usar tanto en `GridSearchCV` como en `cross_val_score`. Simplemente se proporciona  una cadena que describa la métrica de evaluación que deseas usar.\n",
    "\n",
    "Digamos, por ejemplo, que queremos evaluar el clasificador SVM en la tarea \"nueve vs. no nueve\" en el conjunto de datos `digits`, usando la puntuación AUC. Cambiar el puntaje del valor predeterminado (exactitud) a AUC se puede hacer proporcionando `roc_auc` como el parámetro `scoring`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Puntuacion predeterminado para la clasificación es la precisión\n",
    "print(\"Puntuacion predeterminado: {}\".format(\n",
    "    cross_val_score(SVC(), digitos.data, digitos.target == 9)))\n",
    "\n",
    "# Si scoring=\"precision\" no cambia los resultados\n",
    "precision_explicita = cross_val_score(SVC(), digitos.data, digitos.target == 9, \n",
    "                            scoring=\"accuracy\")\n",
    "print(\"Puntuacion de precision explicita: {}\".format(precision_explicita))\n",
    "roc_auc = cross_val_score(SVC(), digitos.data, digitos.target == 9,\n",
    "                          scoring=\"roc_auc\")\n",
    "print(\"Puntuacion AUC: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos cambiar la métrica utilizada para elegir los mejores parámetros en `Gridsearchcv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(\n",
    "digitos.data, digitos.target == 9, random_state=0)\n",
    "\n",
    "# proporcionamos un grid algo mala  para ilustrar el punto:\n",
    "parametros_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# utilizando la puntuacion predeterminada de precision:\n",
    "grid = GridSearchCV(SVC(), param_grid=parametros_grid)\n",
    "grid.fit(X_entrenamiento, y_entrenamiento)\n",
    "print(\"Grid-Search con precision:\")\n",
    "print(\"Mejores parametros:\", grid.best_params_)\n",
    "print(\"Mejor puntuacion de validación cruzada (precision)): {:.3f}\".format(grid.best_score_))\n",
    "print(\"Conjunto de prueba AUC: {:.3f}\".format(\n",
    "roc_auc_score(y_prueba, grid.decision_function(X_prueba))))\n",
    "print(\"Precision del conjunto de prueba: {:.3f}\".format(grid.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la puntuación AUC en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(), param_grid=parametros_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_entrenamiento, y_entrenamiento)\n",
    "print(\"Grid-Search con AUC:\")\n",
    "print(\"Mejores parametros:\", grid.best_params_)\n",
    "print(\"Mejor puntuacion de validación cruzada (precision)): {:.3f}\".format(grid.best_score_))\n",
    "print(\"Conjunto de prueba AUC: {:.3f}\".format(\n",
    "roc_auc_score(y_prueba, grid.decision_function(X_prueba))))\n",
    "print(\"Precision del conjunto de prueba: {:.3f}\".format(grid.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se utiliza la exactitud, se selecciona el parámetro `gamma = 0.0001`, mientras que `gamma = 0.01` se selecciona cuando se usa AUC. La exactitud de validación cruzada es consistente con la exactitud del conjunto de prueba en ambos casos. Sin embargo, el uso de AUC encontró una mejor configuración de parámetros en términos de AUC e incluso en términos de exactitud.\n",
    "\n",
    "Encontrar una solución de mayor precisión con AUC es probablemente una consecuencia de que la precisión sea una mala medida del rendimiento del modelo en datos desbalanceados.\n",
    "\n",
    "Los valores más importantes para el parámetro `scoring ` para la clasificación son `accuracy` (predeterminado); `roc_auc` para el área bajo la curva ROC; `average_precision` para el área bajo la curva de exhaustividad-precisión; `f1`, `f1_macro`, `f1_micro` y `f1_weighted` para el `f1-score ` binario y las diferentes variantes ponderadas. Para la regresión, los valores más comúnmente utilizados son `r2` para el puntaje $R^2$, `mean_squared_error` para el error cuadrático medio y `mean_absolute_error` para el error absoluto medio. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
