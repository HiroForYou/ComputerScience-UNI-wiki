{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curso Inteligencia Artificial\n",
    "\n",
    "El presente texto ha sido preparado de manera exclusiva para los alumnos del Curso de Inteligencia\n",
    "Artificial, que forma parte de la Plan de Estudio de la Escuela de Ciencia de Computación, según el artículo\n",
    "°44 de la Ley sobre el Derecho de Autor, D.L. N°822. Queda prohibida su difusión y reproducción por\n",
    "cualquier medio o procedimiento, total o parcialmente fuera del marco del presente curso.\n",
    "\n",
    "\n",
    "### Métricas de evaluación y puntuaciones\n",
    "\n",
    "Hasta ahora, hemos evaluado el rendimiento de una  clasificación usando la exactitud(la fracción de muestras correctamente clasificadas) y el rendimiento de la regresión usando $R^2$ o $MSE$. Sin embargo, estas son solo dos de las muchas formas posibles de indicar qué tan bien se desempeña un modelo supervisado en un conjunto de datos determinado. En la práctica, estas medidas de evaluación pueden no ser apropiadas para su aplicación  y es importante elegir la métrica correcta al seleccionar entre modelos y ajustar los parámetros.\n",
    "\n",
    "Al seleccionar una métrica, siempre debes tener en cuenta el objetivo final de la aplicación del aprendizaje automático. En la práctica, generalmente nos interesa no solo hacer predicciones precisas, sino también usar estas predicciones como parte de un proceso de toma de decisiones más amplio. \n",
    "\n",
    "Antes de elegir una métrica se debe pensar en el objetivo de alto nivel de la aplicación, a menudo denominado métrica de negocio. Las consecuencias de elegir un algoritmo particular para una aplicación de aprendizaje automático  es llamado impacto de negocios.\n",
    "\n",
    "En este cuaderno, primero discutiremos las métricas para el caso especial de clasificación binaria, luego daremos vuelta a la clasificación multiclase y finalmente a la regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de clasificación binaria \n",
    "\n",
    "La clasificación binaria es posiblemente la aplicación más común y conceptualmente más simple del aprendizaje automático. Sin embargo, todavía hay una serie de advertencias al evaluar incluso esta simple tarea. Antes de sumergirnos en métricas alternativas, echemos un vistazo a las formas en que la medición de la exactitud  puede ser engañosa. Se debe recordar que para una clasificación binaria, se habla de una clase positiva y una clase negativo, con el entendimiento de que la clase positiva es la que estamos buscando.\n",
    "\n",
    "### Tipos de errores\n",
    "\n",
    "A menudo, la exactitud no es una buena medida del rendimiento predictivo, ya que la cantidad de errores que cometemos no contiene toda la información que nos interesa. Imagina una aplicación para detectar la detección temprana del cáncer mediante una prueba automatizada. Si la prueba es negativa, se asumirá que el paciente está sano, mientras que si la prueba es positiva, el paciente se someterá a exámenes adicionales. Aquí, llamaríamos a una prueba positiva (una indicación de cáncer) la clase positiva y una prueba negativa la clase negativa. No podemos suponer que nuestro modelo siempre funcionará perfectamente y cometerá errores. Para cualquier aplicación, debemos preguntarnos cuáles podrían ser las consecuencias de estos errores en el mundo real.\n",
    "\n",
    "Un posible error es que un paciente sano será clasificado como positivo, lo que lleva a pruebas adicionales. Esto conlleva algunos costos y un inconveniente para el paciente (y posiblemente algún problema mental). Una predicción positiva incorrecta se llama un `falso positivo`. El otro posible error es que un paciente enfermo se clasificará como negativo y no recibirá más pruebas y tratamiento. El cáncer no diagnosticado podría causar problemas graves de salud e incluso podría ser fatal. Un error de este tipo, una predicción negativa incorrecta, se denomina `falso negativo`. En las estadísticas, un falso positivo también se conoce como `error tipo I` y un falso negativo como `error tipo II`. Nos apegaremos a `falso negativo` y `falso positivo`, ya que son más explícitos y fáciles de recordar.\n",
    "\n",
    "En el ejemplo del diagnóstico de cáncer, queremos evitar `falsos negativos` tanto como sea posible, mientras que los `falsos positivos` se pueden ver como una molestia menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos de datos no balanceados\n",
    "\n",
    "Los tipos de errores juegan un papel importante cuando una de dos clases es mucho más frecuente que la otra. Esto es muy común en la práctica, un buen ejemplo es la predicción de `clics`, donde cada punto de datos representa una \"impresión\", de elemento que se mostró a un usuario. Este elemento podría ser un anuncio, una historia relacionada o una persona a seguir en un sitio de redes sociales. \n",
    "\n",
    "El objetivo es predecir si, si se muestra un elemento en particular, un usuario hará clic(lo que indica que está interesado). La mayoría de las cosas que los usuarios ven en internet (en particular, los anuncios) no generan un clic. Es posible que debas mostrarle a un usuario $100$ anuncios o artículos antes de que se encuentre algo lo suficientemente interesante como para hacer clic. Esto da como resultado un conjunto de datos donde para cada $99$ puntos de datos *sin clic*, hay $1$ punto de datos *hecho clic*, en otras palabras el $ 99\\%$ de las muestras pertenecen a la clase *sin clic*.\n",
    "\n",
    "Los conjuntos de datos en los que una clase es mucho más frecuente que la otra a menudo se denominan conjuntos de datos no balanceados o conjuntos de datos con clases no balanceadas. En realidad, los datos no balanceados son la norma y es raro que los eventos de interés tengan una frecuencia igual o incluso similar en los datos.\n",
    "\n",
    "Ahora supongamos que crea un clasificador que es $99\\%$ exactitud en la tarea de predicción de clics. ¿Qué te dice eso? El $99\\%$ de exactitud suena impresionante, pero esto no tiene en cuenta las clases no balaceadas. Puede lograr el $99\\%$ de exactitud sin crear un modelo de aprendizaje automático, al predecir siempre \"sin clic\". Por otro lado, incluso con datos no balanceados, un modelo con un $99\\%$ de exactitud podría ser bastante bueno. Sin embargo, la exactitud no nos permite distinguir el modelo constante de \"no hacer clic\" de un modelo potencialmente bueno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Crearemos un conjunto de datos desequilibrado $9: 1$ del conjunto de datos `digits`, al clasificar el dígito $9$ frente a las otras nueve clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digitos = load_digits()\n",
    "y = digitos.target == 9\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(\n",
    "    digitos.data, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el `DummyClassifier` para predecir siempre la clase mayoritaria (aquí \"no nueve\") para ver qué tan desinformativa puede ser la exactitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf_dummy = DummyClassifier(strategy='most_frequent').fit(X_entrenamiento, y_entrenamiento)\n",
    "pred_mas_frecuente = clf_dummy.predict(X_prueba)\n",
    "print(\"Etiquetas predecidas unicas: {}\".format(np.unique(pred_mas_frecuente)))\n",
    "print(\"Puntuacion del conjunto de pruebas: {:.2f}\".format(clf_dummy.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtuvimos cerca del $90\\%$ de exactitud sin aprender nada. Esto puede parecer sorprendente, pero piénselo un momento. Imagina que alguien le dice que su modelo es $90\\%$ exactitud. Podrías pensar que hicieron un muy buen trabajo. ¡Pero dependiendo del problema, eso podría ser posible simplemente prediciendo una clase! Comparemos esto con el uso de un clasificador real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_arbol = DecisionTreeClassifier(max_depth=2).fit(X_entrenamiento, y_entrenamiento)\n",
    "pred_arbol = clf_arbol.predict(X_prueba)\n",
    "print(\"Puntuacion de prueba: {:.2f}\".format(clf_arbol.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el valor de la exactitud obtenida, el `DecisionTreeClassifier`, es ligeramente mejor que el predictor constante. Esto podría indicar que algo está mal con la forma en que usamos el clasificador  `DecisionTreeClassifier`, o que la exactitud de hecho no es una buena medida aquí.\n",
    "\n",
    "Para propósitos de comparación, evaluemos dos clasificadores más, `LogisticRegression` y el predeterminado `DummyClassifier`, que hace predicciones aleatorias pero produce clases con las mismas proporciones que en el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_dummy = DummyClassifier().fit(X_entrenamiento, y_entrenamiento)\n",
    "pred_dummy = clf_dummy.predict(X_prueba)\n",
    "print(\"Puntuacion dummy : {:.2f}\".format(clf_dummy.score(X_prueba, y_prueba)))\n",
    "logreg = LogisticRegression(C=0.1).fit(X_entrenamiento, y_entrenamiento)\n",
    "pred_logreg = logreg.predict(X_prueba)\n",
    "print(\"Puntuacion logreg : {:.2f}\".format(logreg.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador dummy que produce resultados aleatorios es claramente el peor del lote (de acuerdo con la exactitud), mientras que `LogisticRegression` produce muy buenos resultados. Sin embargo, incluso el clasificador aleatorio produce más del $80\\%$ de precisión. Esto hace que sea muy difícil juzgar cuál de estos resultados es realmente útil. El problema aquí es que la exactitud es una medida inadecuada para cuantificar el rendimiento predictivo en este entorno no balanceado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión\n",
    "\n",
    "Una de las formas más completas de representar el resultado de evaluar la clasificación binaria es usar matrices de confusión. Examinemos las predicciones de `LogisticRegression` de la sección anterior usando la función `confusion_matrix`. Ya almacenamos las predicciones en el conjunto de prueba en `pred_logreg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matriz_confusion = confusion_matrix(y_prueba, pred_logreg)\n",
    "print(\"Matriz de confusion:\\n{}\".format(matriz_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de `confusion_matrix` es una matriz de dos por dos, donde las filas corresponden a las clases verdaderas y las columnas corresponden a las clases predichas. Cada entrada cuenta la frecuencia con que una muestra  pertenece a la clase correspondiente a la fila (aquí, \"no nueve\" y \"nueve\") se clasificó como la clase correspondiente a la columna. La siguiente gráfica ilustra este significado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotsMetrics import *\n",
    "dibujo_ilustracion_matriz_confusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas en la diagonal principal de la matriz de confusión corresponden a las clasificaciones correctas, mientras que las otras entradas nos dicen cuántas muestras de una clase se clasificaron erróneamente como otra clase.\n",
    "\n",
    "Si declaramos \"un nueve\" como la clase positiva, podemos relacionar las entradas de la matriz de confusión con los términos falso positivo y falso negativo que presentamos anteriormente. Para completar la imagen, llamamos a las muestras correctamente clasificadas pertenecientes a la clase positiva, `verdaderos positivos` y a las muestras correctamente clasificadas pertenecientes a la clase negativa, `verdaderos negativos`. Estos términos generalmente se abrevian `FP`, `FN`, `TP` y `TN` y conducen a la siguiente interpretación para la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibuja_matriz_confusion_binaria()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos la matriz de confusión para comparar los modelos que ajustamos anteriormente (los dos modelos dummy,  árboles de decisión y la regresión logística):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clases mas frecuentes:\")\n",
    "print(confusion_matrix(y_prueba, pred_mas_frecuente))\n",
    "print(\"\\nModelo dummy:\")\n",
    "print(confusion_matrix(y_prueba, pred_dummy))\n",
    "print(\"\\nArboles de decision:\")\n",
    "print(confusion_matrix(y_prueba, pred_arbol))\n",
    "print(\"\\nRegresion logistica\")\n",
    "print(confusion_matrix(y_prueba, pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar la matriz de confusión, notamos que algo anda mal con `pred_mas_frecuente`, porque siempre predice la misma clase. `pred_dummy`, por otro lado, tiene un número muy pequeño de verdaderos positivos (5), particularmente en comparación con el número de falsos negativos y falsos positivos: ¡hay muchos más falsos positivos que verdaderos positivos! Las predicciones hechas por el árbol de decisiones tienen mucho más sentido que las predicciones dummies, aunque la exactitud era casi la misma. Podemos ver también  que la regresión logística es mejor que `pred_arbol` en todos los aspectos: tiene más verdaderos positivos y verdaderos negativos, mientras que tiene menos falsos positivos y falsos negativos. De esta comparación, solo  los árboles  de decisión y la regresión logística dan resultados razonables y que la regresión logística funciona mejor que el árbol de decisión en todas las cuentas.\n",
    "\n",
    "Sin embargo, inspeccionar la matriz de confusión completa es un poco engorroso, y si bien obtuvimos una gran cantidad de información al observar todos los aspectos de la matriz, el proceso fue muy manual y cualitativo. Hay varias formas de resumir la información en la matriz de confusión, que discutiremos a continuación: \n",
    "\n",
    " $exactitud = \\frac{TP +TN}{TP +TN +FP +FN}$\n",
    "    \n",
    "En otras palabras, la exactitud es el número de predicciones correctas (TP y TN) dividido por el número de todas las muestras (todas las entradas de la matriz de confusión se suman).\n",
    "\n",
    "Hay muchas otras maneras de resumir la matriz de confusión, siendo las más comunes la precisión y la exhaustividad. La precisión mide cuántas de las muestras predichas como positivas son realmente positivas:\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "La precisión se usa como una métrica de rendimiento cuando el objetivo es limitar el número de falsos positivos.\n",
    "\n",
    "La exahustividad, mide cuántas de las muestras positivas son capturadas por las predicciones positivas:\n",
    " \n",
    " $exhaustividad = \\frac{TP}{TP +FN}$\n",
    " \n",
    "Se utiliza como métrica de rendimiento cuando necesitamos identificar todas las muestras positivas, es decir, cuando es importante evitar falsos negativos.\n",
    "\n",
    "Existe una compensación  entre optimizar la exhaustividad y optimizar la precisión. Se puede obtener trivialmente una exhaustividad perfecta si se predice que todas las muestras pertenecen a la clase positiva y no hay falsos negativos ni verdaderos negativos. Sin embargo, predecir todas las muestras como positivas dará como resultado muchos falsos positivos y por lo tanto, la precisión será muy baja.\n",
    "\n",
    "Por otro lado, si se  encuentra un modelo que predice solo el punto de datos único que está más seguro como positivo y el resto como negativo, entonces la precisión será perfecta (suponiendo que este punto de datos sea de hecho positivo) y tendremos una mala exhaustividad.\n",
    "\n",
    "Aunque la precisión y la exhaustividad son medidas muy importantes, utilizar solo a uno de ellas no le proporcionará una imagen completa. Una forma de resumirlos es con el  `f-score` o `f-measure`, que es la media armónica de la precisión y la exhaustividad:\n",
    "\n",
    "$F =  2 \\cdot \\frac{\\text{precision}\\cdot \\text{exhaustividad}}{\\text{precision} + \\text{exhaustividad}}$\n",
    "\n",
    "Como esta medida tiene en cuenta la precisión y la exhaustividad, puede ser una mejor medida que la precisión en conjuntos de datos de clasificación binaria desbalanceados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"Puntuacion f1 mas frecuente: {:.2f}\".format(f1_score(y_prueba, pred_mas_frecuente)))\n",
    "print(\"Puntuacion f1 de un clasificador dummy: {:.2f}\".format(f1_score(y_prueba, pred_dummy)))\n",
    "print(\"Puntuacion f1 de un arbol de decision : {:.2f}\".format(f1_score(y_prueba, pred_arbol)))\n",
    "print(\"Puntuacion f1 para la regresion logistica : {:.2f}\".format(f1_score(y_prueba, pred_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar dos cosas aquí. Primero, obtenemos un mensaje de error para `pred_mas_frecuente`, ya que no hubo predicciones de la clase positiva (lo que hace que el denominador en `f-score` sea cero). Además, podemos ver una distinción bastante fuerte entre `pred_summy` y `pred_arbol`, que no estaba clara cuando se analiza solo precisión.\n",
    "\n",
    "Usar `f-score` para la evaluación de modelos, se resume en un número. Sin embargo, `f-score` parece capturar nuestra intuición de lo que hace que un buen modelo sea mucho mejor que la precisión. Una desventaja del `f-score`, sin embargo, es que es más difícil de interpretar y explicar que la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos un resumen más completo de precisión, exhaustividad y f-score, podemos usar la función `classification_report` para calcular los tres valores  a la vez e imprimirlos en un formato agradable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_prueba, pred_mas_frecuente, target_names=[\"no nueve\", \"nueve\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `class_report` produce una línea por clase (aquí, True y False) e informa la precisión, la exahustividad y el f-score con esta clase como clase positiva. Antes, asumíamos que la clase minoritaria “nueve” era la clase positiva. Si cambiamos la clase positiva a \"no nueve\", podemos ver a partir de la salida de `clasificación_report` que obtenemos un f-score $0.94$ con el modelo `mas_frecuente`. Además, para la clase \"no nueve\" tenemos una exahustividad de $1$, ya que clasificamos todas las muestras como \"no nueve\". La última columna proporciona el `support` de cada clase, lo que  significa el número de muestras en cada clase de acuerdo con la observaciones empíricas.\n",
    "\n",
    "La última fila  muestra un promedio ponderado (por el número de muestras en la clase) de los números para cada clase. Aquí hay dos reportes más, uno para el clasificador dummy y el otro para la regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_prueba, pred_dummy, target_names=[\"no nueve\", \"nueve\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_prueba, pred_logreg, target_names=[\"no nueve\", \"nueve\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar al mirar los reportes, las diferencias entre el modelo dummy  y un modelo muy bueno ya no son tan claros. Escoger qué clase es declarada como la clase positiva tiene un gran impacto en las métricas. Mientras que el `f-score` para la clasificación dummy es $0.11$ (frente a $0.92$ para la regresión logística) en la clase \"nueve\", para la clase \"no nueve\" es $0.90$ frente a $0.99$, que parecen resultados razonables.\n",
    "\n",
    "Al observar todos los números en conjunto, se dibuja una imagen bastante precisa, y podemos ver claramente la superioridad del modelo de regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teniendo en cuenta la incertidumbre\n",
    "\n",
    "La matriz de confusión y `classification_report`,  proporcionan un análisis muy detallado de un conjunto particular de predicciones. Sin embargo, las mismas predicciones ya arrojaron mucha información que está contenida en el modelo. La mayoría de los clasificadores proporcionan  una `funcion_decision` o un método de `predict_proba` para evaluar los grados de certeza sobre las predicciones. Hacer predicciones puede verse como el umbral de la salida de `funcion_decision` o `predict_proba` en un cierto punto fijo; en la clasificación binaria usamos $0$ para la función de decisión y $0.5$ para `predict_proba`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "El siguiente es un ejemplo de una tarea de clasificación binaria desbalanceada, con $400$ puntos en la clase negativa clasificados contra $50$ puntos en la clase positiva. Los datos de entrenamiento se muestran a la izquierda de la siguiente figura generada. Formamos un modelo SVM de kernel con estos datos y los gráficos a la derecha de los datos de entrenamiento ilustran los valores de la función de decisión como un mapa de calor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = hacer_blobs(n_muestras=(400, 50), centros=2, cluster_std=[7.0, 2],\n",
    "                      random_state=22)\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_entrenamiento, y_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibuja_umbral_decision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la función `classification_report` para evaluar la precisión y exhaustividad para ambas clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_prueba, svc.predict(X_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la clase $1$, obtenemos un `recall` bastante pequeño y la precisión es mixta. Debido a que la clase $0$ es mucho más grande, el clasificador se enfoca en hacer que la clase $0$ sea correcta  y no  en clase $1$ más pequeña.\n",
    "\n",
    "Supongamos en nuestra aplicación es más importante tener la exahustividad o `recall` elevado para la clase $1$, como en el ejemplo de detección de cáncer anterior. Esto significa que estamos dispuestos a arriesgar más falsos positivos a cambio de más positivos verdaderos (lo que aumentará la exhaustividad). Las predicciones generadas por `svc.predict` realmente no cumplen este requisito, pero podemos ajustar las predicciones para centrarnos en  un valor de exhaustividad más alto de la clase $1$ al cambiar el umbral de decisión de $0$. Por defecto, los puntos con un valor de `function_decision` mayor que $0$ deben clasificarse como clase $1$. Si queremos que más puntos se clasifiquen como clase $1$ debemos disminuir el umbral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_menor_umbral = svc.decision_function(X_prueba) > -.8\n",
    "print(classification_report(y_prueba, y_pred_menor_umbral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esperaba, el `recall` de la clase $1$ subió y la precisión bajó. Ahora estamos clasificando una región de espacio más grande como clase $1$, como se ilustra en el lado superior derecho de la figura anterior. Si se valora la precisión sobre la exhaustividad o al revés, o si sus datos están muy desbalanceados, cambiar el umbral de decisión es la forma más fácil de obtener mejores resultados. Como la función `decision_function` puede tener rangos arbitrarios, es difícil proporcionar una regla general sobre cómo elegir un umbral.\n",
    "\n",
    "Si establece un umbral, se debe tener cuidado de no hacerlo utilizando el conjunto de prueba. Como con cualquier otro parámetro, establecer un umbral de decisión en el conjunto de prueba es probable  a que arroje resultados demasiado optimistas. Se debe utilizar un conjunto de validación o validación cruzada en su lugar.\n",
    "\n",
    "Elegir un umbral para los modelos que implementan el método `predict_proba` puede ser más fácil, ya que la salida de `predict_proba` está en una escala fija de $0$ a $1$. Por defecto, el umbral de $0.5$ significa que si el modelo tiene más del $50\\%$ de \"seguridad\" de que un punto es de la clase positiva, se clasificará como tal. Aumentar el umbral significa que el modelo necesita tener más confianza para tomar una decisión positiva (y menos confianza para tomar una decisión negativa).\n",
    "\n",
    "Si bien trabajar con probabilidades puede ser más intuitivo que trabajar con umbrales arbitrarios, no todos los modelos proporcionan modelos realistas de incertidumbre. Esto se relaciona con el concepto de calibración: un modelo calibrado es un modelo que proporciona una medida precisa de su incertidumbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
