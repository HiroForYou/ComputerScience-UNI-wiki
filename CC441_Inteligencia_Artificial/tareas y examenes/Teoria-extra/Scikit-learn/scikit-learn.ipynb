{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas de Scikit-Learn\n",
    "\n",
    "Material basado en el libro de Jake VanderPlas: https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (aprendizaje Automático), es una rama  de la inteligencia artificial que estudia el diseño de algoritmos que puedan aprender. \n",
    "\n",
    "Típicas tareas del machine learning son el aprendizaje de conceptos,el desarrollo de  modelos de predicción,  la búsqueda de patrones predictivos y el clustering. Estas tareas son aprendidas  a través de los datos disponibles observados  a través de experiencias o instrucciones. Podemos dividir los algoritmos  del machine learning en 3 categorias:\n",
    "\n",
    "- **Aprendizaje supervisado**\n",
    "- **Aprendizaje no supervisado**\n",
    "- **Aprendizaje por refuerzo**\n",
    " \n",
    "[scikit](https://scikits.appspot.com/)  es un conjunto de paquetes en Python, para temas específicos:\n",
    "\n",
    "- scikit-image: Contiene rutina para procesamiento de imágenes en Scipy.\n",
    "- scikit-monaco: Paquete relacionado a la integración de Montecarlo.\n",
    "- timeseries: Un paquete para la manipulación de series de tiempo.\n",
    "- scikit-learn: Un conjunto de módulos en Python, para Machine Learning y Data Minning.\n",
    "\n",
    "La librería  [scikit-learn](scikit-learn.org/stable/a) proporciona una gran número de herramientas para la ciencia de datos y data minning en Python centrandose en el machine learning:\n",
    "\n",
    "- scikit-learn posee excelente documentación como el  [tutorial](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) inicial o los [ejemplos](http://scikit-learn.org/dev/auto_examples/index.html).\n",
    "- Posee muchos de lo algoritmos del machine learning.\n",
    "- Todo se trabaja sobre [github](https://github.com/scikit-learn/scikit-learn).\n",
    "- Cada método implementado sobre sckit-learn asume que la data viene en un conjunto de datos. Sckit-learn incluye algunos conjuntos de datos conocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrenamiento, validación y de prueba\n",
    "\n",
    "\n",
    "- Los datos de entrenamiento(**training**): son los datos que entrenan los modelos\n",
    "- Los datos de validación (**validation**) : elecciona el mejor de los modelos entrenados.\n",
    "- Los datos de prueba (**testing**): Nos ofrece el error real cometido con el modelo seleccionado.\n",
    "\n",
    "[Hastie, Tibshirani y Friedman](http://www-stat.stanford.edu/~tibs/ElemStatLearn/) en su libro *The Elements of Statistical Learning* indican que una división típica para estos conjuntos puede ser de 50% para el entrenamiento y 25% para la validación y prueba, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API de scikit-learn\n",
    "\n",
    "Los principales \"interfaces\" en scikit-learn son (una clase puede implementar múltiples interfaces):\n",
    "\n",
    "- **Estimador**:    Un estimador es un objeto que se ajuste a un modelo basado en algunos datos de entrenamiento y es capaz de inferir algunas propiedades de los nuevos datos.\n",
    "\n",
    "    `estimador = obj.fit(data, targets)` \n",
    "    \n",
    " \n",
    "- **Predictor**:\n",
    "\n",
    "    `prediccion = obj.predict(data)` \n",
    "    \n",
    "    \n",
    "- **Transformador**:\n",
    "\n",
    "    `n_data = obj.transform(data)` \n",
    "    \n",
    "    \n",
    "- **Modelo**:\n",
    "\n",
    "    `s = obj.score(data)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from numpy import logical_or\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sgd_separador \n",
    "import regresion \n",
    "\n",
    "iris = datasets.load_iris()\n",
    "subconjunto = logical_or(iris.target == 0, iris.target == 1)\n",
    "\n",
    "X = iris.data[subconjunto]\n",
    "y = iris.target[subconjunto]\n",
    "\n",
    "# Creamos el modelo\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fijamos el modelo\n",
    "model.fit(X, y)\n",
    "print (model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para aprendizaje no supervisado\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Creamos el modelo\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "\n",
    "# Fijamos el modelo\n",
    "kmeans.fit(X)\n",
    "print (kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduccion de la dimension\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creamos el modelo\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fijamos el modelo\n",
    "pca.fit(X)\n",
    "print (pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictores\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "X_test = [[ 5.006,  3.418,  1.464,  0.244], [ 5.936,  2.77 ,  4.26 ,  1.326]]\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo más avanzado de SVM,  usando SVC (support vector classification)\n",
    "\n",
    "from sklearn import svm\n",
    "digitos = datasets.load_digits()\n",
    "clasificador = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# Fijamos el modelo\n",
    "\n",
    "clasificador.fit(digitos.data[:-1], digitos.target[:-1])  \n",
    "\n",
    "# Usamos los predictores en el modelo\n",
    "\n",
    "clasificador.predict(digitos.data[-1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El metodo transform para aprendizaje no supervisado\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "print (pca.transform(X)[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El metodo fit.transform\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "print (pca.fit_transform(X)[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo sobre el metodo Model\n",
    "\n",
    "# Utilizamos DummyClassifier un clasificador que hace predicciones simples\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model = DummyClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.score(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos de las tareas más simples del machine learning es la de la `clasificación` y de  la `regresión`. Veamos el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó la primera  tarea es la de `clasificación`: la figura de arriba  muestra un conjunto de datos de dos dimensiones, coloreadas de acuerdo a dos clase etiquetadas  diferentes. Un algoritmo de clasificación se puede utilizar para trazar una frontera divisoria entre los dos grupos de puntos. Al dibujar esta línea de separación, hemos aprendido un modelo que se puede generalizar a nuevos datos: si tuviera que soltar  un punto en el plano que no está etiquetado, este algoritmo podría predecir si se trata de un punto azul o  rojo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra tarea del machine learning es la de `regressión`, que es el ajuste de una línea a un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, este es un ejemplo de ajuste de un modelo a los datos, de tal manera que el modelo puede hacer generalizaciones acerca de nuevos datos. El modelo que ha aprendido de los datos de entrenamiento, se puede utilizar para predecir el resultado de datos de prueba: aquí, se podría dar un valor x, y el modelo nos permitiría predecir el valor de y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos en machine learning\n",
    "\n",
    "### Aprendizaje supervisado\n",
    "\n",
    "\n",
    "Estos algoritmos consisten en una variable de salida/resultado (o variable dependiente) que ha de ser predicha a partir de un conjunto dado de predictores (variables independientes). Usando  este conjunto de variables, generamos una función que se asignan las entradas a las salidas deseadas. El proceso de formación continúa hasta que el modelo alcanza un nivel deseado de precisión en los datos de entrenamiento. Algunos ejemplos:\n",
    "\n",
    "- [**Linear Regression**](https://jeremykun.com/2013/08/18/linear-regression/).\n",
    "- [**Logistic Regression**](http://blog.yhat.com/posts/logistic-regression-and-python.html).\n",
    "- [**KNN**](https://jeremykun.com/2012/08/26/k-nearest-neighbors-and-handwritten-digit-classification/).\n",
    "- [**Random Forest**](http://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/).\n",
    "\n",
    "### Aprendizaje no supervisado\n",
    "\n",
    "En estos algoritmos, no tenemos ninguna variable objetivo o resultado para predecir/estimar. Se utiliza para la agrupación de la población en  diferentes grupos, lo que  es ampliamente utilizado para la segmentación de clientes en diferentes grupos por intervención específica. Algunos ejemplos :\n",
    "\n",
    "- [**K-Means**](https://jeremykun.com/2012/10/08/decision-trees-and-political-party-classification/).\n",
    "- [**Dimensionality Reduction Algorithms**](http://www.kdnuggets.com/2015/05/7-methods-data-dimensionality-reduction.html).\n",
    "\n",
    "### Aprendizaje por refuerzo\n",
    "\n",
    "Utilizando este algoritmo, la máquina está capacitada para tomar decisiones específicas. Funciona de la siguiente manera: la máquina está expuesta a un ambiente donde se entrena a sí misma continuamente mediante ensayo y error. Esta máquina aprende de la experiencia pasada y trata de captar el mejor conocimiento posible para tomar decisiones de negocio precisos. Un ejemplo:\n",
    "\n",
    "- Procesos de decisión de Markov.\n",
    "- Metodos de Montecarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresion lineal\n",
    "from sklearn.linear_model import LinearRegression \n",
    "# Creando la data\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(30, 1))\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(30)\n",
    "\n",
    "# Fijamos el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Dibujamos los datos y el modelo de prediccion\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijemos un arbol aleatorio\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Dibujamos los datos y el modelo de prediccion\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Validación de modelos\n",
    "\n",
    "Una pieza importante de machine learning  es la `validación del modelo`: es decir, determinar qué tan bien el modelo puede generalizar a partir de los datos de entrenamiento futuros datos no etiquetados. Veamos un ejemplo usando el `clasificador de vecino más cercano (nearest neighbor classifier)`. Este es un clasificador muy simple: lo que hace es  almacenar todos los datos de entrenamiento, y para cualquier cantidad desconocida, simplemente devuelve la etiqueta del punto de entrenamiento más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el conjunto de datos iris, la prediccion de los datos de entrada\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = iris.data, iris.target\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(np.all(y == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma más útil examinar los resultados es para ver la `matriz de confusión (confusión matrix)`, o la matriz que muestra la frecuencia de las entradas y salidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada clase, los 50 muestras de entrenamiento se identifican correctamente, pero en el fondo este modelo generaliza mal a los nuevos datos. Podemos simular esto mediante el fraccionamiento de nuestros datos en un conjunto de entrenamiento y un conjunto de pruebas. Scikit-learn contiene algunas rutinas convenientes para hacer esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xentrenamiento, Xtest, yentrenamiento, ytest = train_test_split(X, y)\n",
    "clf.fit(Xentrenamiento, yentrenamiento)\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto muestra verdadero rendimiento de nuestro clasificador: al parecer hay cierta confusión entre la segunda y la tercera especie, que podría anticipar dado lo que hemos visto de los datos anteriores.\n",
    "\n",
    "Por esta razón, es muy importante utilizar  los datos de  `entrenamiento` y datos `test` en la evaluación de los  modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos iris\n",
    "\n",
    "Como se ha mencionado scikit-learn tiene un conjunto de datos de la especie `iris`, cuyos datos consisten de lo siguiente:\n",
    "\n",
    "- Características del conjunto de datos iris\n",
    " \n",
    "  1.  sepal length in cm\n",
    "  2.  sepal width in cm\n",
    "  3.  petal length in cm\n",
    "  4.  petal width in cm\n",
    "\n",
    "- Clases a predecir\n",
    "\n",
    "    1.  Iris Setosa\n",
    "    2.  Iris Versicolour\n",
    "    3.  Iris Virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()  # copia del archivo csv del conjunto de dato iris\n",
    "\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los algoritmos de machine learning implementados en scikit-learn esperan que los datos se almacenen en una matriz . Las matrices pueden ser tanto matrices de  `numpy`, o en algunos casos matrices de `scipy.sparse`.\n",
    "\n",
    "Se espera que el tamaño de la matriz sea  `[n_muestras, n_caracteristicas]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_muestras, n_caracteristicas = iris.data.shape\n",
    "print((n_muestras, n_caracteristicas))\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La data es de 4 dimensiones, pero se visualizar en dos , usando matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "\n",
    "# este formateador etiquetará la barra de colores con los nombres  de los target correctos\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo que resume y agrega algunas caracteristicas adicionales\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X_iris, Y_iris = iris.data, iris.target\n",
    "print (X_iris.shape, Y_iris.shape)\n",
    "print(X_iris[0], Y_iris[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos tiene 150 instancias y 4 atributos. En nuestro primer paso, separamos el conjunto de datos , usando el %75 de la instancias para 'entrenar' nuestro clasificador y el 25% para evaluarlos (estamos usando dos características: sepal width y length).\n",
    "\n",
    "También calculamos el promedio, restamos el valor medio desde el valor caracteristico y dividimos el resultado por la desviacion estandar, lo que produce que cada característica tenga una media cero y una desviación estándar de uno. La `estandarización de valores` es muy común en machine learning, evitan que las características con grandes valores puedan ponderar demasiado los resultados finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Conseguimos el conjunto de datos solo con los dos primeros\n",
    "# atributos\n",
    "\n",
    "X,Y =X_iris[:, :2], Y_iris\n",
    "\n",
    "# Dividir el conjunto de datos en conjuntos de entrenamiento \n",
    "# testing\n",
    "\n",
    "X_entrenamiento, X_test, Y_entrenamiento, Y_test \\\n",
    "= train_test_split(X, Y, test_size = 0.25, random_state=34)\n",
    "print(X_entrenamiento.shape, Y_entrenamiento.shape)\n",
    "\n",
    "# Estandarizacion\n",
    "\n",
    "scaler = StandardScaler().fit(X_entrenamiento)\n",
    "X_entrenamiento = scaler.transform(X_entrenamiento)\n",
    "\n",
    "X_test =scaler.transform(X_test)\n",
    "\n",
    "# Graficando los datos de entrenamiento usando matplolib\n",
    "\n",
    "colores = ['red', 'green', 'blue']\n",
    "for i in range(len(colores)):\n",
    "    px =X_entrenamiento[:,0][Y_entrenamiento == i]\n",
    "    py =X_entrenamiento[:, 1][Y_entrenamiento ==i]\n",
    "    plt.scatter(px, py, c =colores[i])\n",
    "\n",
    "plt.legend(iris.target_names)\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\");\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La setosa es facilmente separable de las otras dos clases mientras que versicolor y virginica estan dispersos entre ellos. Para implementar una clasificacion lineal, usamos el SGDClassifier desde Scikit-learn.\n",
    "\n",
    "SGD es el método del descenso de gradiente estocástico que es muy útil para encontrar mínimos locales de una función. En este caso encontramos  un método de clasificación lineal y los coeficientes del hiperplano que minimiza  la distancia entre los puntos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un modelo lineal clasificatorio\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "\n",
    "#fijamos el clasificador (entrenamiento)\n",
    "clf.fit(X_entrenamiento, Y_entrenamiento)\n",
    "\n",
    "# Imprimimos los coeficientes del hiperplano que minimiza \n",
    "# loss function(funcion de perdida)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "\n",
    "# Dibujar las 3 curvas de decision \n",
    "\n",
    "x_min, x_max = X_entrenamiento[:,0].min() -.5, X_entrenamiento[:,0].max() + .5\n",
    "y_min, y_max = X_entrenamiento[:,1].min() -.5, X_entrenamiento[:,1].max() + .5\n",
    "\n",
    "xs = np.arange(x_min, x_max, 0.5)\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_size_inches(10,6)\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    axes[i].set_aspect(\"equal\")\n",
    "    axes[i].set_title('Clase' + str(i) + 'versus el resto')\n",
    "    axes[i].set_xlim(x_min, x_max)\n",
    "    axes[i].set_ylim(y_min, y_max)\n",
    "    axes[i].set_xlabel(\"Longitud Sepal\")\n",
    "    axes[i].set_ylabel(\"Ancho Sepal\")\n",
    "    \n",
    "    plt.sca(axes[i])\n",
    "    \n",
    "    for j in range(len(colores)):\n",
    "        px =X_entrenamiento[:,0][Y_entrenamiento == j]\n",
    "        py =X_entrenamiento[:, 1][Y_entrenamiento ==j]\n",
    "        plt.scatter(px, py, c =colores[j])\n",
    "\n",
    "    ys = (-clf.intercept_[i] -xs*clf.coef_[i,0])/clf.coef_[i,1]\n",
    "    plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El clasificador puede predecir la clase de cierta instancia , dado la\n",
    "# longitud  y ancho de sepal\n",
    "\n",
    "print(clf.predict(scaler.transform([[4.7, 3.1]])))\n",
    "print(clf.decision_function(scaler.transform([[4.7, 3.1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
