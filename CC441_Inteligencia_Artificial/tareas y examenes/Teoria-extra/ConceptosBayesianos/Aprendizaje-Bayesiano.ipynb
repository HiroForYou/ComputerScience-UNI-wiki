{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos de aprendizaje  Bayesiano\n",
    "\n",
    "Referencia: Machine Learning-a Probabilistic Approach Kevin Murphy 2012.\n",
    "\n",
    "Considera la tarea de aprender un concepto en el siguiente sentido: a un aprendiz se le muestra una secuencia de instancias (objetos) etiquetados como pertenecientes a un concepto. Por ejemplo, a un niño se le muestra un animal y se le dice que es un *perro* o un *gato*. Los objetos pueden pertenecer a múltiples conceptos. Esperamos que el aprendiz pueda generalizar sobre la base de la descripción percibida de las instancias, de modo que las instancias no vistas puedan clasificarse en el conjunto de conceptos.\n",
    "\n",
    "Modelamos esta tarea como la de aprender una función $f(instancia)$ mapeando instancias a valores booleanos - $f (x) = 1$ indica que $x$ pertenece al concepto representado por $f$, $f(x) = 0$ indica que $x$ no pertenece al concepto.\n",
    "\n",
    "El siguiente ejemplo de esta tarea de aprendizaje de conceptos se denomina *number game* y fue introducido por Tenenbaum en 1999 en su tesis doctoral. En este juego, las instancias son un conjunto finito de números enteros (por ejemplo, $\\{1,2,\\dots, 100\\}$). Los conceptos se describen mediante expresiones de lenguaje natural, por ejemplo, *números primos* o *números pares* o *potencias de 2*. Estos denotan cada uno un subconjunto de las instancias. Por ejemplo los *números pares* denota el subconjunto $\\{2,4,6, \\dots,98,100\\}$.\n",
    "\n",
    "El proceso de aprendizaje de conceptos se modela como una actualización incremental del modelo Bayesiano. El escenario del aprendizaje se ilustra con este ejemplo:\n",
    "\n",
    "```\n",
    "Quiero enseñarte un \"concepto\" que es un conjunto de números entre 1 y 100.\n",
    "Te digo que \"16\" es un ejemplo positivo del concepto.\n",
    "¿Qué otros números crees que son positivos?\n",
    "¿Es \"17\"? \"6\"? \"32\"? \"99\"?\n",
    "\n",
    "```\n",
    "\n",
    "Naturalmente, con un solo ejemplo es difícil de responder, por lo que las predicciones serán vagas. Cuando decidamos la predicción, preferiremos números que sean \"similares\" a \"16\", pero ¿similares en qué sentido? \"17\" es similar porque está \"cerca\" de \"16\", \"6\" es similar porque tiene un dígito en común con \"16\", \"32\" porque también es par y también una potencia de \"2\", \"99\" no parece similar por ninguna \"buena razón\".\n",
    "\n",
    "Por lo tanto, algunos números son más probables que otros y podemos representar nuestras predicciones como una distribución de probabilidad $p(\\tilde{x}|D)$, donde $\\tilde{x}$ es el número sobre el cual necesitamos hacer una predicción (\"32\" o \"99\") y $D$ es el conjunto de datos que hemos observado hasta ahora, inicialmente solo {\"16\"}. Esto se llama **distribución predictiva posterior**.\n",
    "\n",
    "Podemos observar esta distribución predictiva posterior preguntando a un grupo de personas su predicción sobre todos los números entre $1$ y $100$ dados los ejemplos enseñados hasta ahora en forma de histograma. La siguiente figura (reproducida en Murphy 2012 desde el trabajo Tenenbaum 1999): muestra la distribución predictiva empírica promediada sobre las predicciones de $8$ humanos en el *number game*.\n",
    "\n",
    "\n",
    "* Las dos primeras filas son las predicciones después de ver D = {16} y D = {60}. Esto ilustra una similitud difusa.\n",
    "* La tercera fila después de ver D = {16,8,2,64}. Esto ilustra un comportamiento similar a una regla (potencias de 2).\n",
    "* La última fila después de ver D = {16,23,19,20}. Esto ilustra una similitud enfocada (números cercanos a 20).\n",
    "\n",
    "![](Number-game.png)\n",
    "\n",
    "Suponga que observamos ejemplos positivos {\"16\", \"8\", \"2\", \"64\"} (como en la fila 3). La gente tiende a concluir que el concepto es el conjunto de \"potencias de 2\" (como se indica con los picos en el histograma que reflejan el juicio de $8$ personas). Este es un ejemplo de **inducción**.\n",
    "\n",
    "Nuestra tarea es emular este comportamiento inductivo/de generalización utilizando un modelo matemático. El enfoque consiste en definir un espacio de hipótesis de conceptos, un conjunto $H$ de subconjuntos de números. Por ejemplo, números impares, números pares, todos los números, potencias de dos, números que terminan en $j$ ($j$ en $[0..9]$). El subconjunto de $H$ que es consistente con $D$ se denomina **espacio de versión**. A medida que obtenemos más ejemplos en $D$, el espacio de la versión se vuelve más pequeño y nos volvemos más seguros de nuestras predicciones.\n",
    "\n",
    "Esta historia, sin embargo, no es suficiente para explicar el comportamiento de generalización observado. Vemos que preferimos algunos conceptos sobre otros (por ejemplo, dado $D = \\{16,8,2,64\\}$ preferimos \"potencia de 2\" sobre \"números pares\" o \"todos los números\"). Desarrollaremos un modelo Bayesiano para explicar estas preferencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood (verosimilitud)\n",
    "\n",
    "Debemos explicar por qué preferimos una hipótesis (potencia de dos) sobre otra dado el conjunto de datos $D$. La intuición es que queremos evitar coincidencias sospechosas: si el concepto verdadero es \"números pares\", ¿por qué solo vimos potencias de dos?\n",
    "\n",
    "El modelo que adoptamos es: supongamos que muestreamos los datos observados en $D$ a partir de la extensión del concepto verdadero según una distribución uniforme. (La extensión de un concepto es el conjunto de instancias que pertenecen al concepto, por ejemplo $h_{par} = \\{2,4,6,\\dots, 98,100 \\}$, la extensión de números que terminan en $9$ es $\\{9, 19, \\dots, 99 \\}$) Ten en cuenta que esto es psicológicamente una suposición poco probable, porque sabemos que la gente prefiere \"ejemplos típicos\". \n",
    "\n",
    "Dada esta suposición, la probabilidad de muestrear $N$ elementos con reemplazo desde $h$ es:\n",
    "\n",
    "$$p(D|h) = (1/\\vert h\\vert)^N \\quad (donde N = \\vert D\\vert)$$\n",
    "\n",
    "Esta ecuación crucial incorpora lo que Tenenbaum llama el principio del tamaño, lo que significa que el modelo favorece la hipótesis más simple (más pequeña) consistente con los datos. Esto se conoce más comúnmente como la navaja de Occam.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "$$p(\\{16\\}|h_{potencia2}) = 1/6 \\quad  (\\text{desde que}\\ h_{potencia2}=\\{2,4,8,16,32,64\\} \\text{tiene 6 elementos}), \\quad p(\\{16\\} | h_{par}) = 1/50$$ \n",
    "\n",
    "Con $4$ ejemplos:\n",
    "\n",
    "$$p(\\{16,4,2,32\\}|h_{potencia2}) = (1/6)^4 = 7.7\\times 10^{-4}, \\quad p(\\{16,4,2,32\\}|h_{par}) = (1/50)^4 = 1.6 \\times 10^{-7}$$-\n",
    "\n",
    "La tasa de verosimilitud es $5000:1$ en favor de $h_{potencia2}$. Esto cuantifica nuestra intuición anterior de que $D = \\{16, 8, 2, 64\\}$ sería una coincidencia muy sospechosa si se generara por $h_{par}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "\n",
    "En una perspectiva Bayesiana, queremos actualizar nuestras creencias anteriores dadas nuevas observaciones. Esto significa que también debemos modelar nuestras creencias previas sobre la probabilidad de los conceptos en el espacio de hipótesis.\n",
    "\n",
    "Por ejemplo, consideramos $D = \\{16,8,2,64\\}$. Dado D, la hipótesis del concepto *h'= \"potencias de 2 excepto 32\"* es más probable que h = \"potencias de 2\" (dada la fórmula de probabilidad presentada).\n",
    "\n",
    "*h'*, sin embargo, no parece un \"concepto natural\". Queremos modelar nuestra preferencia a priori por $h$ sobre $h'$. Esta clasificación de preferencias es difícil de justificar, se basa en criterios subjetivos. En este modelo, preparamos un espacio de hipótesis de unos $30$ conceptos y los clasificamos:\n",
    "\n",
    "* Números pares\n",
    "* Números impares\n",
    "* Cuadrados\n",
    "* Múltiplos de 3\n",
    "* ...\n",
    "* Múltiplos de 9\n",
    "* Múltiplos de 10\n",
    "* Termina en 1\n",
    "* ...\n",
    "* Termina en 9\n",
    "* potencias de 2\n",
    "* ...\n",
    "* Potencias de 10\n",
    "* Toda  \n",
    "* Potencias de 2 + {37}\n",
    "* Potencias de 2 - {32}\n",
    "\n",
    "Damos una probabilidad previa más alta para los pares y los impares, todos los demás tienen la misma probabilidad, excepto los 2 últimos conceptos \"no naturales\" a los que asignamos una probabilidad muy baja.\n",
    "\n",
    "\n",
    "![](Prior-posterior.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "\n",
    "La distribución posterior $p(h|D)$ estima la probabilidad de cada concepto de hipótesis $h$ dada la observación D. Por la regla de Bayes, el posterior es proporcional a la verosimilitud multiplicada por el prior, normalizado por todos los posteriores:\n",
    "\n",
    "$$p(h|D) = \\frac{p(D|h)p(h)}{\\sum_{h^{'} \\in h}p(D, h^{'})}$$\n",
    "\n",
    "Usando la definición de probabilidad anterior:\n",
    "\n",
    "$$p(h|D) = \\frac{p(h)I(D \\in h)/\\vert h\\vert^N }{\\sum_{h^{'} \\in h}p(h^{'})I(D \\in h^{'})/\\vert h^{'} \\vert^N}$$\n",
    "\n",
    "donde $I(D \\in h)$ es $1$ si todos los elementos en $D$ están en la extensión de la hipótesis $h$, $0$ en caso contrario.\n",
    "\n",
    "La figura anterior dibuja el previo, la verosimilitud y el posterior después de ver $D = \\{16\\}$. Vemos que el posterior es una combinación del previo y verosimilitud. En el caso de la mayoría de los conceptos, el previo es uniforme, por lo que el posterior es proporcional a la verosimilitud.  Sin embargo, los conceptos como  \"potencias de 2, más 37\" y \"potencias de 2, excepto 32\" tienen un soporte posterior bajo, a pesar de tener una alta verosimilitud, debido al previo bajo. Por el contrario, el concepto de 'números impares' tiene un  posterior bajo, a pesar de tener un previo alto, debido a la baja verosimilitud.\n",
    "\n",
    "En general, cuando tenemos suficientes datos, el posterior $p(h|D)$ alcanza su punto máximo en un solo concepto, a saber, la estimación MAP, es decir,\n",
    "\n",
    "$$p(h|D) \\rightarrow \\delta_{\\hat{h}^{MAP}}(h)$$\n",
    "\n",
    "\n",
    "donde $\\hat{h}^{MAP} = arg \\max_{h}p(h|D)$ es el modo posterior y $\\delta $ es la medida de Dirac, definida como:\n",
    "\n",
    "$$\\delta_x(A) = 1\\ \\text{si}\\  x \\in A; 0\\ \\text{si}\\ x \\notin A.$$\n",
    "\n",
    "\n",
    "La estimación MAP puede ser escrita como:\n",
    "\n",
    "$$\\hat{h}^{MAP} = arg \\max_{h}p(D|h)p(h) =  arg \\max_{h}[\\log p(D|h) + \\log p(h)]$$\n",
    "\n",
    "\n",
    "Dado que el término de probabilidad depende exponencialmente de $N$, y el previo permanece constante, a medida que obtenemos más y más datos, la estimación de MAP converge hacia la estimación de máxima verosimilitud o MLE:\n",
    "\n",
    "$$\\hat{h}^{MLE}= arg \\max_{h}p(D|h)= arg \\max_{h}\\log p(D|h)$$\n",
    "\n",
    "\n",
    "En otras palabras, si tenemos suficientes datos, vemos que los datos 'sobrepasa' a los previos. En este caso, la estimación de MAP converge hacia el MLE.\n",
    "\n",
    "Si la hipótesis verdadera está en el espacio de hipótesis, entonces la estimación de MAP/ML convergerá en esta hipótesis. Por lo tanto, decimos que la inferencia bayesiana (y la estimación ML) son estimadores consistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución predictiva posterior\n",
    "\n",
    "El posterior es nuestro estado interno de creencias sobre el mundo. La forma de probar si nuestras creencias están justificadas es usarlas para predecir cantidades objetivamente observables (esta es la base del método científico). Específicamente, la distribución predictiva posterior en este contexto viene dada por:\n",
    "\n",
    "\n",
    "$$p(\\tilde{x} \\in C|D) = \\sum_{h}p(y =1|\\tilde{x}, h)p(h|D)$$\n",
    "\n",
    "Este es solo un promedio ponderado de las predicciones de cada hipótesis individual y se denomina promedio del modelo de Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un previo más complejo\n",
    "\n",
    "Para modelar el comportamiento humano, Tenenbaum utilizó un previo ligeramente más sofisticado que se derivó del análisis de algunos datos experimentales de cómo las personas miden la similitud entre números. El resultado es un conjunto de conceptos aritméticos similares a los mencionados anteriormente, más todos los intervalos entre $n$ y $m$ para $1 \\leq n, m \\leq 100$. (Ten en cuenta que estas hipótesis no son mutuamente excluyentes). Por lo tanto, el previo es una mezcla de dos previos, uno sobre reglas aritméticas y uno sobre intervalos:\n",
    "\n",
    "$$p(h) = \\pi_{0}p_{reglas}(h) + (1- \\pi_{0}) p_{intervalo}(h)$$\n",
    "\n",
    "El único parámetro libre en el modelo es el peso relativo, $\\pi_0$, dado a estas dos partes del previo. Los resultados no son muy sensibles a este valor, siempre que $\\pi_0> 0.5$, lo que refleja el hecho de que es más probable que las personas piensen en conceptos definidos por reglas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador Naive Bayes\n",
    "\n",
    "Naive Bayes es una técnica de clasificación estadística basada en el Teorema de Bayes. Es uno de los algoritmos de aprendizaje supervisado más simples. El clasificador Naive Bayes es un algoritmo rápido, preciso y confiable. Los clasificadores Naive Bayes tienen alta exactitud y velocidad en grandes conjuntos de datos.\n",
    "\n",
    "El clasificador Naive Bayes asume que el efecto de una característica particular en una clase es independiente de otras características. Por ejemplo, un solicitante de préstamo es deseable o no dependiendo de sus ingresos, historial de préstamos y transacciones anteriores, edad y ubicación. Incluso si estas características son interdependientes, estas características aún se consideran de forma independiente. Esta suposición simplifica el cálculo y por eso se considera ingenua.\n",
    "\n",
    "Referencia: [https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "col_names = ['etiqueta', 'mensaje']\n",
    "sms = pd.read_table('sms.tsv', sep='\\t', header=None, names=col_names)\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: etiqueta, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.etiqueta.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['etiqueta'] = sms.etiqueta.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sms.mensaje\n",
    "y = sms.etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train.shape\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizando la data\n",
    "\n",
    "`CountVectorizer` es una gran herramienta proporcionada por scikit-learn y se utiliza para transformar un texto dado en un vector sobre la base de la frecuencia de cada palabra que aparece en todo el texto a través de una matriz en la que cada palabra única está representada por una columna de la matriz  y cada muestra de texto del documento es una fila en la matriz. El valor de cada celda no es más que la frecuencia (conteo) de la palabra en esa muestra de texto en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examinamos los tokens y los conteos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '008704050406',\n",
       " '0121',\n",
       " '01223585236',\n",
       " '01223585334',\n",
       " '0125698789',\n",
       " '02',\n",
       " '0207',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '021',\n",
       " '03',\n",
       " '04',\n",
       " '0430',\n",
       " '05',\n",
       " '050703',\n",
       " '0578',\n",
       " '06']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos cuántas veces aparece cada token en todos los mensajes en `X_train_dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 23,  2, ...,  1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train_counts = np.sum(X_train_dtm.toarray(), axis=0)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7456,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>conteo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>jules</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>mallika</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>malarky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>makiing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>maintaining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>in</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>and</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6542</th>\n",
       "      <td>the</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7420</th>\n",
       "      <td>you</td>\n",
       "      <td>1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>to</td>\n",
       "      <td>1670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token  conteo\n",
       "3727        jules       1\n",
       "4172      mallika       1\n",
       "4169      malarky       1\n",
       "4165      makiing       1\n",
       "4161  maintaining       1\n",
       "...           ...     ...\n",
       "3502           in     683\n",
       "929           and     717\n",
       "6542          the    1004\n",
       "7420          you    1660\n",
       "6656           to    1670\n",
       "\n",
       "[7456 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'token':X_train_tokens, 'conteo':X_train_counts}).sort_values('conteo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos el \"spam\" de cada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_ham = sms[sms.etiqueta==0]\n",
    "sms_spam = sms[sms.etiqueta==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(sms.mensaje)\n",
    "todos_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos matrices de documentos para ham y spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dtm = vect.transform(sms_ham.mensaje)\n",
    "spam_dtm = vect.transform(sms_spam.mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4825x8713 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57836 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_conteos = np.sum(ham_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_conteos = np.sum(spam_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_conteos = pd.DataFrame({'token':todos_tokens, 'ham':ham_conteos, 'spam':spam_conteos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_conteos['ham'] = token_conteos.ham + 1\n",
    "token_conteos['spam'] = token_conteos.spam + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el radio de spam-ham para cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>radio_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>gt</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>lt</td>\n",
       "      <td>317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3805</th>\n",
       "      <td>he</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6843</th>\n",
       "      <td>she</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>lor</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>tone</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>150p</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>prize</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>claim</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8713 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  ham  spam  radio_spam\n",
       "3684     gt  319     1    0.003135\n",
       "4793     lt  317     1    0.003155\n",
       "3805     he  232     1    0.004310\n",
       "6843    she  168     1    0.005952\n",
       "4747    lor  163     1    0.006135\n",
       "...     ...  ...   ...         ...\n",
       "369      18    1    52   52.000000\n",
       "7837   tone    1    61   61.000000\n",
       "352    150p    1    72   72.000000\n",
       "6113  prize    1    94   94.000000\n",
       "2067  claim    1   114  114.000000\n",
       "\n",
       "[8713 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_conteos['radio_spam'] = token_conteos.spam/token_conteos.ham\n",
    "token_conteos.sort_values('radio_spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construyendo de un modelo Naive Bayes\n",
    "\n",
    "El clasificador multinomial Naive Bayes es adecuado para la clasificación con características discretas (por ejemplo, el conteo de palabras para la clasificación de texto). La distribución multinomial normalmente requiere conteos de características enteras. Sin embargo, en la práctica, los conteos fraccionarios como **tf-idf** también pueden funcionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(sum(y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885139985642498"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de confusión es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1203,    5],\n",
       "       [  11,  174]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos las probabilidades (mal calibradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87744864e-03, 1.83488846e-05, 2.07301295e-03, ...,\n",
       "       1.09026171e-06, 1.00000000e+00, 3.98279868e-09])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866431000536962"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el texto del mensaje para los falsos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574               Waiting for your call.\n",
       "3375             Also andros ice etc etc\n",
       "45      No calls..messages..missed calls\n",
       "3415             No pic. Please re-send.\n",
       "1988    No calls..messages..missed calls\n",
       "Name: mensaje, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el texto del mensaje para los falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132    LookAtMe!: Thanks for your purchase of a video...\n",
       "5       FreeMsg Hey there darling it's been 3 week's n...\n",
       "3530    Xmas & New Years Eve tickets are now on sale f...\n",
       "684     Hi I'm sue. I am 20 years old and work as a la...\n",
       "1875    Would you like to see my XXX pics they are so ...\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "4298    thesmszone.com lets you send free anonymous an...\n",
       "4949    Hi this is Amy, we will be sending you a free ...\n",
       "2821    INTERFLORA - It's not too late to order Inter...\n",
       "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "4514    Money i have won wining number 946 wot do i do...\n",
       "Name: mensaje, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[y_test > y_pred_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué notas sobre los falsos negativos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando el clasificador Naive Bayes con otros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892318736539842"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923035618399859"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Lee el código de [nltk.classify.naivebayes](http://www.nltk.org/_modules/nltk/classify/naivebayes.html) y ejecuta  `nltk.classify.naivebayes.demo()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\HiroFoerYou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# Tu solucion\n",
    "# Natural Language Toolkit: Naive Bayes Classifiers\n",
    "#\n",
    "# Copyright (C) 2001-2020 NLTK Project\n",
    "# Author: Edward Loper <edloper@gmail.com>\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\"\"\"\n",
    "A classifier based on the Naive Bayes algorithm.  In order to find the\n",
    "probability for a label, this algorithm first uses the Bayes rule to\n",
    "express P(label|features) in terms of P(label) and P(features|label):\n",
    "\n",
    "|                       P(label) * P(features|label)\n",
    "|  P(label|features) = ------------------------------\n",
    "|                              P(features)\n",
    "\n",
    "The algorithm then makes the 'naive' assumption that all features are\n",
    "independent, given the label:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                                         P(features)\n",
    "\n",
    "Rather than computing P(features) explicitly, the algorithm just\n",
    "calculates the numerator for each label, and normalizes them so they\n",
    "sum to one:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                        SUM[l]( P(l) * P(f1|l) * ... * P(fn|l) )\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs\n",
    "from nltk.classify.api import ClassifierI\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "\n",
    "##//////////////////////////////////////////////////////\n",
    "##  Naive Bayes Classifier\n",
    "##//////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier(ClassifierI):\n",
    "    \"\"\"\n",
    "    A Naive Bayes classifier.  Naive Bayes classifiers are\n",
    "    paramaterized by two probability distributions:\n",
    "\n",
    "      - P(label) gives the probability that an input will receive each\n",
    "        label, given no information about the input's features.\n",
    "\n",
    "      - P(fname=fval|label) gives the probability that a given feature\n",
    "        (fname) will receive a given value (fval), given that the\n",
    "        label (label).\n",
    "\n",
    "    If the classifier encounters an input with a feature that has\n",
    "    never been seen with any label, then rather than assigning a\n",
    "    probability of 0 to all labels, it will ignore that feature.\n",
    "\n",
    "    The feature value 'None' is reserved for unseen feature values;\n",
    "    you generally should not use 'None' as a feature value for one of\n",
    "    your own features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_probdist, feature_probdist):\n",
    "        \"\"\"\n",
    "        :param label_probdist: P(label), the probability distribution\n",
    "            over labels.  It is expressed as a ``ProbDistI`` whose\n",
    "            samples are labels.  I.e., P(label) =\n",
    "            ``label_probdist.prob(label)``.\n",
    "\n",
    "        :param feature_probdist: P(fname=fval|label), the probability\n",
    "            distribution for feature values, given labels.  It is\n",
    "            expressed as a dictionary whose keys are ``(label, fname)``\n",
    "            pairs and whose values are ``ProbDistI`` objects over feature\n",
    "            values.  I.e., P(fname=fval|label) =\n",
    "            ``feature_probdist[label,fname].prob(fval)``.  If a given\n",
    "            ``(label,fname)`` is not a key in ``feature_probdist``, then\n",
    "            it is assumed that the corresponding P(fname=fval|label)\n",
    "            is 0 for all values of ``fval``.\n",
    "        \"\"\"\n",
    "        self._label_probdist = label_probdist\n",
    "        self._feature_probdist = feature_probdist\n",
    "        self._labels = list(label_probdist.samples())\n",
    "\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "\n",
    "    def classify(self, featureset):\n",
    "        return self.prob_classify(featureset).max()\n",
    "\n",
    "\n",
    "    def prob_classify(self, featureset):\n",
    "        # Discard any feature names that we've never seen before.\n",
    "        # Otherwise, we'll just assign a probability of 0 to\n",
    "        # everything.\n",
    "        featureset = featureset.copy()\n",
    "        for fname in list(featureset.keys()):\n",
    "            for label in self._labels:\n",
    "                if (label, fname) in self._feature_probdist:\n",
    "                    break\n",
    "            else:\n",
    "                # print('Ignoring unseen feature %s' % fname)\n",
    "                del featureset[fname]\n",
    "\n",
    "        # Find the log probabilty of each label, given the features.\n",
    "        # Start with the log probability of the label itself.\n",
    "        logprob = {}\n",
    "        for label in self._labels:\n",
    "            logprob[label] = self._label_probdist.logprob(label)\n",
    "\n",
    "        # Then add in the log probability of features given labels.\n",
    "        for label in self._labels:\n",
    "            for (fname, fval) in featureset.items():\n",
    "                if (label, fname) in self._feature_probdist:\n",
    "                    feature_probs = self._feature_probdist[label, fname]\n",
    "                    logprob[label] += feature_probs.logprob(fval)\n",
    "                else:\n",
    "                    # nb: This case will never come up if the\n",
    "                    # classifier was created by\n",
    "                    # NaiveBayesClassifier.train().\n",
    "                    logprob[label] += sum_logs([])  # = -INF.\n",
    "\n",
    "        return DictionaryProbDist(logprob, normalize=True, log=True)\n",
    "\n",
    "\n",
    "    def show_most_informative_features(self, n=10):\n",
    "        # Determine the most relevant features, and display them.\n",
    "        cpdist = self._feature_probdist\n",
    "        print(\"Most Informative Features\")\n",
    "\n",
    "        for (fname, fval) in self.most_informative_features(n):\n",
    "\n",
    "            def labelprob(l):\n",
    "                return cpdist[l, fname].prob(fval)\n",
    "\n",
    "            labels = sorted(\n",
    "                [l for l in self._labels if fval in cpdist[l, fname].samples()],\n",
    "                key=lambda element: (-labelprob(element), element),\n",
    "                reverse=True\n",
    "            )\n",
    "            if len(labels) == 1:\n",
    "                continue\n",
    "            l0 = labels[0]\n",
    "            l1 = labels[-1]\n",
    "            if cpdist[l0, fname].prob(fval) == 0:\n",
    "                ratio = \"INF\"\n",
    "            else:\n",
    "                ratio = \"%8.1f\" % (\n",
    "                    cpdist[l1, fname].prob(fval) / cpdist[l0, fname].prob(fval)\n",
    "                )\n",
    "            print(\n",
    "                (\n",
    "                    \"%24s = %-14r %6s : %-6s = %s : 1.0\"\n",
    "                    % (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    def most_informative_features(self, n=100):\n",
    "        \"\"\"\n",
    "        Return a list of the 'most informative' features used by this\n",
    "        classifier.  For the purpose of this function, the\n",
    "        informativeness of a feature ``(fname,fval)`` is equal to the\n",
    "        highest value of P(fname=fval|label), for any label, divided by\n",
    "        the lowest value of P(fname=fval|label), for any label:\n",
    "\n",
    "        |  max[ P(fname=fval|label1) / P(fname=fval|label2) ]\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"_most_informative_features\"):\n",
    "            return self._most_informative_features[:n]\n",
    "        else:\n",
    "            # The set of (fname, fval) pairs used by this classifier.\n",
    "            features = set()\n",
    "            # The max & min probability associated w/ each (fname, fval)\n",
    "            # pair.  Maps (fname,fval) -> float.\n",
    "            maxprob = defaultdict(lambda: 0.0)\n",
    "            minprob = defaultdict(lambda: 1.0)\n",
    "\n",
    "            for (label, fname), probdist in self._feature_probdist.items():\n",
    "                for fval in probdist.samples():\n",
    "                    feature = (fname, fval)\n",
    "                    features.add(feature)\n",
    "                    p = probdist.prob(fval)\n",
    "                    maxprob[feature] = max(p, maxprob[feature])\n",
    "                    minprob[feature] = min(p, minprob[feature])\n",
    "                    if minprob[feature] == 0:\n",
    "                        features.discard(feature)\n",
    "\n",
    "            # Convert features to a list, & sort it by how informative\n",
    "            # features are.\n",
    "            self._most_informative_features = sorted(\n",
    "                features, key=lambda feature_: (minprob[feature_] / maxprob[feature_], feature_[0],\n",
    "                                                feature_[1] in [None, False, True], str(feature_[1]).lower())\n",
    "            )\n",
    "        return self._most_informative_features[:n]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def train(cls, labeled_featuresets, estimator=ELEProbDist):\n",
    "        \"\"\"\n",
    "        :param labeled_featuresets: A list of classified featuresets,\n",
    "            i.e., a list of tuples ``(featureset, label)``.\n",
    "        \"\"\"\n",
    "        label_freqdist = FreqDist()\n",
    "        feature_freqdist = defaultdict(FreqDist)\n",
    "        feature_values = defaultdict(set)\n",
    "        fnames = set()\n",
    "\n",
    "        # Count up how many times each feature value occurred, given\n",
    "        # the label and featurename.\n",
    "        for featureset, label in labeled_featuresets:\n",
    "            label_freqdist[label] += 1\n",
    "            for fname, fval in featureset.items():\n",
    "                # Increment freq(fval|label, fname)\n",
    "                feature_freqdist[label, fname][fval] += 1\n",
    "                # Record that fname can take the value fval.\n",
    "                feature_values[fname].add(fval)\n",
    "                # Keep a list of all feature names.\n",
    "                fnames.add(fname)\n",
    "\n",
    "        # If a feature didn't have a value given for an instance, then\n",
    "        # we assume that it gets the implicit value 'None.'  This loop\n",
    "        # counts up the number of 'missing' feature values for each\n",
    "        # (label,fname) pair, and increments the count of the fval\n",
    "        # 'None' by that amount.\n",
    "        for label in label_freqdist:\n",
    "            num_samples = label_freqdist[label]\n",
    "            for fname in fnames:\n",
    "                count = feature_freqdist[label, fname].N()\n",
    "                # Only add a None key when necessary, i.e. if there are\n",
    "                # any samples with feature 'fname' missing.\n",
    "                if num_samples - count > 0:\n",
    "                    feature_freqdist[label, fname][None] += num_samples - count\n",
    "                    feature_values[fname].add(None)\n",
    "\n",
    "        # Create the P(label) distribution\n",
    "        label_probdist = estimator(label_freqdist)\n",
    "\n",
    "        # Create the P(fval|label, fname) distribution\n",
    "        feature_probdist = {}\n",
    "        for ((label, fname), freqdist) in feature_freqdist.items():\n",
    "            probdist = estimator(freqdist, bins=len(feature_values[fname]))\n",
    "            feature_probdist[label, fname] = probdist\n",
    "\n",
    "        return cls(label_probdist, feature_probdist)\n",
    "\n",
    "\n",
    "\n",
    "##//////////////////////////////////////////////////////\n",
    "##  Demo\n",
    "##//////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "def demo():\n",
    "    from nltk.classify.util import names_demo\n",
    "\n",
    "    classifier = names_demo(NaiveBayesClassifier.train)\n",
    "    classifier.show_most_informative_features()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Testing classifier...\n",
      "Accuracy: 0.7820\n",
      "Avg. log likelihood: -0.7476\n",
      "\n",
      "Unseen Names      P(Male)  P(Female)\n",
      "----------------------------------------\n",
      "  Kelli            0.0132  *0.9868\n",
      "  Er              *0.8826   0.1174\n",
      "  Ally             0.0903  *0.9097\n",
      "  Stephan         *0.8361   0.1639\n",
      "  Chriss           0.6864  *0.3136\n",
      "Most Informative Features\n",
      "                endswith = 'a'            female : male   =     31.5 : 1.0\n",
      "                endswith = 'p'              male : female =     14.2 : 1.0\n",
      "                endswith = 'v'              male : female =     13.0 : 1.0\n",
      "                endswith = 'f'              male : female =     10.5 : 1.0\n",
      "                endswith = 'm'              male : female =     10.3 : 1.0\n",
      "                endswith = 'd'              male : female =     10.2 : 1.0\n",
      "                endswith = 'o'              male : female =      7.7 : 1.0\n",
      "                count(v) = 2              female : male   =      6.5 : 1.0\n",
      "                endswith = 'r'              male : female =      6.4 : 1.0\n",
      "                endswith = 'w'              male : female =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio (opcional):** Lee el siguiente artículo de Andrew Y. Ng y Michael I. Jordan: [On Discriminative vs. Generative classifier: A comparison of logical regression and naive Bayes](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
